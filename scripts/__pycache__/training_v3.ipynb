{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SVM: 100%|██████████| 3/3 [01:28<00:00, 29.48s/it]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All resources saved with timestamp: 20250126_222502\n",
      "\n",
      "================================================================================\n",
      "                                 FINAL RESULTS                                  \n",
      "================================================================================\n",
      "+--------------+----------------+------------+------------+------------+--------+\n",
      "|    Model     |   Hamming Loss |  Accuracy  |  Micro F1  |  Macro F1  |    MAE |\n",
      "+==============+================+============+============+============+========+\n",
      "| Naive Bayes  |         0.0017 |   57.37%   |   67.81%   |   3.69%    | 0.0025 |\n",
      "+--------------+----------------+------------+------------+------------+--------+\n",
      "| Logistic Reg |         0.0016 |   58.23%   |   69.27%   |   1.90%    | 0.0033 |\n",
      "+--------------+----------------+------------+------------+------------+--------+\n",
      "|     SVM      |         0.0008 |   80.40%   |   86.47%   |   33.11%   | 0.0012 |\n",
      "+--------------+----------------+------------+------------+------------+--------+\n",
      "Training complete. Models and test data saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, hamming_loss, \n",
    "    mean_absolute_error, precision_score, recall_score\n",
    ")\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "# -------------------- Core Classes --------------------\n",
    "class DataLoader:\n",
    "    @staticmethod\n",
    "    def load_data(file_path: str) -> Tuple[pd.Series, np.ndarray, MultiLabelBinarizer]:\n",
    "        \"\"\"Load and clean data with multi-label support\"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Handle missing values\n",
    "        df = df.dropna(subset=['description', 'categories'])\n",
    "        df['description'] = df['description'].fillna('').str.strip()\n",
    "        \n",
    "        # Convert categories to binary matrix\n",
    "        df['categories'] = df['categories'].astype(str)\n",
    "        df['categories'] = df['categories'].str.split(', ')  # Assuming comma-separated labels\n",
    "        \n",
    "        # Create MultiLabelBinarizer\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        y = mlb.fit_transform(df['categories'])\n",
    "        \n",
    "        return df['description'], y, mlb\n",
    "\n",
    "class Preprocessor(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"Enhanced preprocessor with basic text cleaning\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            max_features=5000,\n",
    "            min_df=2,\n",
    "            max_df=0.95\n",
    "        )\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Convert to pandas Series for consistent handling\n",
    "        X = pd.Series(X).astype(str)\n",
    "        self.vectorizer.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Ensure input is always treated as a pandas Series\n",
    "        X = pd.Series(X).astype(str)\n",
    "        return self.vectorizer.transform(X)\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, models: Dict[str, Any], metrics: Dict[str, Any]):\n",
    "        self.models = models\n",
    "        self.metrics = metrics\n",
    "        self.results = []\n",
    "        \n",
    "    def train_and_evaluate(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"Train and evaluate all models with extended metrics\"\"\"\n",
    "        with tqdm(total=len(self.models), desc=\"Training models\") as model_pbar:\n",
    "            for model_name, model in self.models.items():\n",
    "                model_pbar.set_description(f\"Training {model_name}\")\n",
    "                \n",
    "                # Training\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # Predictions\n",
    "                predictions = model.predict(X_test)\n",
    "                \n",
    "                # Calculate probabilities for MAE (if available)\n",
    "                prob_predictions = (model.predict_proba(X_test) \n",
    "                                    if hasattr(model, 'predict_proba') \n",
    "                                    else None)\n",
    "                \n",
    "                # Calculate all metrics\n",
    "                metrics_results = {}\n",
    "                for metric_name, metric_func in self.metrics.items():\n",
    "                    try:\n",
    "                        if metric_name == 'mae' and prob_predictions is not None:\n",
    "                            metrics_results[metric_name] = mean_absolute_error(\n",
    "                                y_test, prob_predictions[:, 1] if prob_predictions.shape[1] == 2 else prob_predictions\n",
    "                            )\n",
    "                        else:\n",
    "                            metrics_results[metric_name] = metric_func(y_true=y_test, y_pred=predictions)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error calculating {metric_name}: {str(e)}\")\n",
    "                        metrics_results[metric_name] = np.nan\n",
    "                \n",
    "                self.results.append({\n",
    "                    'model': model_name,\n",
    "                    **metrics_results,\n",
    "                    'instance': model\n",
    "                })\n",
    "                model_pbar.update(1)\n",
    "\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        \n",
    "    def run(self, X, y):\n",
    "        \"\"\"Run complete experiment pipeline\"\"\"\n",
    "        # Split data - keep original text data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=self.config['test_size'], random_state=42\n",
    "        )\n",
    "        \n",
    "        # Preprocess data - create transformed versions\n",
    "        preprocessor = self.config['preprocessor']()\n",
    "        X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "        X_test_transformed = preprocessor.transform(X_test)\n",
    "        \n",
    "        # Initialize and run trainer with transformed data\n",
    "        trainer = ModelTrainer(self.config['models'], self.config['metrics'])\n",
    "        trainer.train_and_evaluate(X_train_transformed, X_test_transformed, y_train, y_test)\n",
    "        \n",
    "        # Save models and preprocessor\n",
    "        if 'save_dir' in self.config:\n",
    "            save_models(\n",
    "                trainer.results,\n",
    "                preprocessor,\n",
    "                self.config['mlb'],\n",
    "                self.config['save_dir']\n",
    "            )\n",
    "        \n",
    "        # Return original text data for evaluation\n",
    "        return trainer.results, X_test, y_test\n",
    "\n",
    "\n",
    "# -------------------- Utility Functions --------------------\n",
    "def save_models(results: List[Dict], preprocessor: Any, mlb: MultiLabelBinarizer, save_dir: str):\n",
    "    \"\"\"Save trained models and preprocessor to disk\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    try:\n",
    "        # Save preprocessor and MLB\n",
    "        joblib.dump(preprocessor, f\"{save_dir}/preprocessor_{timestamp}.pkl\")\n",
    "        joblib.dump(mlb, f\"{save_dir}/mlb_{timestamp}.pkl\")\n",
    "        \n",
    "        # Save models\n",
    "        for result in results:\n",
    "            model_name = result['model'].replace(' ', '_').lower()\n",
    "            filename = f\"{save_dir}/{model_name}_{timestamp}.pkl\"\n",
    "            joblib.dump(result['instance'], filename)\n",
    "            \n",
    "        print(f\"\\nAll resources saved with timestamp: {timestamp}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving models: {str(e)}\")\n",
    "\n",
    "def print_results(results: List[Dict], sort_by: str = 'subset_accuracy'):\n",
    "    \"\"\"Print results with pretty formatting\"\"\"\n",
    "    metric_names = {\n",
    "        'hamming_loss': 'Hamming Loss',\n",
    "        'subset_accuracy': 'Accuracy', \n",
    "        'micro_f1': 'Micro F1',\n",
    "        'macro_f1': 'Macro F1',\n",
    "        'mae': 'MAE'\n",
    "    }\n",
    "    \n",
    "    # Prepare table data\n",
    "    headers = [\"Model\"] + list(metric_names.values())\n",
    "    table_data = []\n",
    "    \n",
    "    # sorted_results = sorted(results, key=lambda x: x.get(sort_by, 0), reverse=True)\n",
    "    \n",
    "    for res in results:\n",
    "        row = [res['model']]\n",
    "        for metric in metric_names.keys():\n",
    "            value = res.get(metric, np.nan)\n",
    "            if isinstance(value, float):\n",
    "                if 'Loss' in metric_names[metric] or 'MAE' in metric_names[metric]:\n",
    "                    row.append(f\"{value:.4f}\")\n",
    "                else:\n",
    "                    row.append(f\"{value:.2%}\")\n",
    "            else:\n",
    "                row.append(\"N/A\")\n",
    "        table_data.append(row)\n",
    "    \n",
    "    # Print formatted table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL RESULTS\".center(80))\n",
    "    print(\"=\"*80)\n",
    "    print(tabulate(table_data, headers=headers, tablefmt=\"grid\", stralign=\"center\"))\n",
    "\n",
    "def load_resources(model_path: str, preprocessor_path: str, mlb_path: str) -> tuple:\n",
    "    \"\"\"Load saved model, preprocessor, and label binarizer\"\"\"\n",
    "    try:\n",
    "        return (\n",
    "            joblib.load(model_path),\n",
    "            joblib.load(preprocessor_path),\n",
    "            joblib.load(mlb_path)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading resources: {str(e)}\")\n",
    "\n",
    "def evaluate_loaded_model(model: Any, preprocessor: Any, mlb: MultiLabelBinarizer,\n",
    "                         X_test: pd.Series, y_test: np.ndarray) -> None:\n",
    "    \"\"\"Evaluate and print metrics for a loaded model\"\"\"\n",
    "    # Preprocess test data\n",
    "    X_test_transformed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = model.predict(X_test_transformed)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'hamming_loss': hamming_loss(y_test, predictions),\n",
    "        'subset_accuracy': accuracy_score(y_test, predictions),\n",
    "        'micro_f1': f1_score(y_test, predictions, average='micro'),\n",
    "        'macro_f1': f1_score(y_test, predictions, average='macro'),\n",
    "    }\n",
    "    \n",
    "    # Handle MAE separately\n",
    "    try:\n",
    "        prob_predictions = model.predict_proba(X_test_transformed)\n",
    "        results['mae'] = mean_absolute_error(\n",
    "            y_test, prob_predictions[:, 1] if prob_predictions.shape[1] == 2 else prob_predictions\n",
    "        )\n",
    "    except AttributeError:\n",
    "        results['mae'] = np.nan\n",
    "    \n",
    "    # Print results\n",
    "    print_results([{'model': 'Loaded Model', **results}])\n",
    "\n",
    "def predict_genre(model: Any, preprocessor: Any, mlb: MultiLabelBinarizer,\n",
    "                 synopsis: str, threshold: float = 0.5) -> dict:\n",
    "    \"\"\"Predict genre for a single synopsis\"\"\"\n",
    "    # Preprocess text\n",
    "    processed = preprocessor.transform([synopsis])\n",
    "    \n",
    "    # Make prediction\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probabilities = model.predict_proba(processed)\n",
    "        prediction = (probabilities >= threshold).astype(int)\n",
    "    else:\n",
    "        prediction = model.predict(processed)\n",
    "        probabilities = None\n",
    "    \n",
    "    # Convert to labels\n",
    "    labels = mlb.inverse_transform(prediction)\n",
    "    \n",
    "    return {\n",
    "        'prediction': labels[0],\n",
    "        'probabilities': dict(zip(mlb.classes_, probabilities[0])) if probabilities is not None else None\n",
    "    }\n",
    "\n",
    "# -------------------- Configuration --------------------\n",
    "\n",
    "CONFIG = {\n",
    "    'test_size': 0.2,\n",
    "    'save_dir': \"saved_models\",\n",
    "    'preprocessor': None,\n",
    "    'models': {\n",
    "        'Naive Bayes': OneVsRestClassifier(MultinomialNB()),\n",
    "        'Logistic Reg': OneVsRestClassifier(LogisticRegression(max_iter=1000)),\n",
    "        'SVM': OneVsRestClassifier(SVC(kernel='linear', probability=True))\n",
    "    },\n",
    "    'metrics': {\n",
    "        'hamming_loss': hamming_loss,\n",
    "        'subset_accuracy': accuracy_score,\n",
    "        'micro_f1': lambda y_true, y_pred: f1_score(y_true, y_pred, average='micro'),\n",
    "        'macro_f1': lambda y_true, y_pred: f1_score(y_true, y_pred, average='macro'),\n",
    "        'mae': mean_absolute_error\n",
    "    }\n",
    "}\n",
    "# -------------------- Main Execution --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    X, y, mlb = DataLoader.load_data(\"../data/books_dataset.csv\")\n",
    "    CONFIG['preprocessor'] = Preprocessor\n",
    "    CONFIG['mlb'] = mlb\n",
    "    \n",
    "    # Run experiment\n",
    "    runner = ExperimentRunner(CONFIG)\n",
    "    results, X_test_raw, y_test = runner.run(X, y)  # Now contains original text data\n",
    "    \n",
    "    # Show comparison\n",
    "    print_results(results)\n",
    "    \n",
    "    # Save test data for later evaluation\n",
    "    joblib.dump((X_test_raw, y_test), \"test_data.pkl\")\n",
    "    print(\"Training complete. Models and test data saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Results:\n",
      "+--------------+---------+\n",
      "| Metric       |   Value |\n",
      "+==============+=========+\n",
      "| Hamming Loss |  0.0016 |\n",
      "+--------------+---------+\n",
      "| Accuracy     |  0.5823 |\n",
      "+--------------+---------+\n",
      "| Micro F1     |  0.6927 |\n",
      "+--------------+---------+\n",
      "| Macro F1     |  0.019  |\n",
      "+--------------+---------+\n",
      "| MAE          |  0.0033 |\n",
      "+--------------+---------+\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 111\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Make prediction\u001b[39;00m\n\u001b[1;32m    110\u001b[0m sample_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA space opera featuring interstellar politics, alien species, and epic battles between galactic empires\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 111\u001b[0m \u001b[43mpredict_genre\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 81\u001b[0m, in \u001b[0;36mpredict_genre\u001b[0;34m(model, preprocessor, mlb, text, threshold)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_genre\u001b[39m(model, preprocessor, mlb, text: \u001b[38;5;28mstr\u001b[39m, threshold: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m):\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Interactive prediction function\"\"\"\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict_proba\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     84\u001b[0m         probabilities \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(processed)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[37], line 59\u001b[0m, in \u001b[0;36mPreprocessor.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 59\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "# -------------------- Inference Cell --------------------\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tabulate import tabulate\n",
    "\n",
    "def load_resources(timestamp: str = \"latest\") -> tuple:\n",
    "    \"\"\"Load saved model, preprocessor, and label binarizer\"\"\"\n",
    "    try:\n",
    "        if timestamp == \"latest\":\n",
    "            # Find all preprocessor files\n",
    "            files = [f for f in os.listdir(\"saved_models\") \n",
    "                    if f.startswith(\"preprocessor_\") and f.endswith(\".pkl\")]\n",
    "            \n",
    "            if not files:\n",
    "                raise ValueError(\"No saved models found\")\n",
    "            \n",
    "            # Extract full timestamp from filename (preprocessor_YYYYMMDD_HHMMSS.pkl)\n",
    "            latest_file = sorted(files)[-1]\n",
    "            timestamp = latest_file.split('_')[1] + '_' + latest_file.split('_')[2].split('.')[0]\n",
    "            \n",
    "        # Find matching model file (there might be multiple models)\n",
    "        model_files = [f for f in os.listdir(\"saved_models\") \n",
    "                      if f.endswith(f\"{timestamp}.pkl\") and not f.startswith(('preprocessor_', 'mlb_'))]\n",
    "        \n",
    "        if not model_files:\n",
    "            raise ValueError(f\"No model found for timestamp {timestamp}\")\n",
    "            \n",
    "        # Load resources\n",
    "        return (\n",
    "            joblib.load(f\"saved_models/{model_files[0]}\"),  # Load first matching model\n",
    "            joblib.load(f\"saved_models/preprocessor_{timestamp}.pkl\"),\n",
    "            joblib.load(f\"saved_models/mlb_{timestamp}.pkl\")\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading resources: {str(e)}\")\n",
    "\n",
    "def print_evaluation(results: dict):\n",
    "    \"\"\"Pretty print evaluation metrics\"\"\"\n",
    "    metrics = {\n",
    "        'Hamming Loss': results['hamming_loss'],\n",
    "        'Accuracy': results['subset_accuracy'],\n",
    "        'Micro F1': results['micro_f1'],\n",
    "        'Macro F1': results['macro_f1'],\n",
    "        'MAE': results['mae']\n",
    "    }\n",
    "    \n",
    "    table = [[k, f\"{v:.4f}\" if isinstance(v, float) else v] for k, v in metrics.items()]\n",
    "    print(tabulate(table, headers=[\"Metric\", \"Value\"], tablefmt=\"grid\"))\n",
    "\n",
    "def evaluate_model(model, preprocessor, mlb):\n",
    "    \"\"\"Complete evaluation workflow\"\"\"\n",
    "    try:\n",
    "        # Load original text data and labels\n",
    "        X_test_raw, y_test = joblib.load(\"test_data.pkl\")\n",
    "        \n",
    "        # Transform using the preprocessor\n",
    "        X_test_transformed = preprocessor.transform(X_test_raw)\n",
    "        \n",
    "        predictions = model.predict(X_test_transformed)\n",
    "        \n",
    "        results = {\n",
    "            'hamming_loss': hamming_loss(y_test, predictions),\n",
    "            'subset_accuracy': accuracy_score(y_test, predictions),\n",
    "            'micro_f1': f1_score(y_test, predictions, average='micro'),\n",
    "            'macro_f1': f1_score(y_test, predictions, average='macro'),\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            prob_predictions = model.predict_proba(X_test_transformed)\n",
    "            results['mae'] = mean_absolute_error(y_test, prob_predictions)\n",
    "        except AttributeError:\n",
    "            results['mae'] = \"N/A\"\n",
    "            \n",
    "        print(\"Model Evaluation Results:\")\n",
    "        print_evaluation(results)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Test data not found. Run training cell first.\")\n",
    "\n",
    "def predict_genre(model, preprocessor, mlb, text: str, threshold: float = 0.2):\n",
    "    \"\"\"Interactive prediction function\"\"\"\n",
    "    processed = preprocessor.transform([text])\n",
    "    \n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probabilities = model.predict_proba(processed)\n",
    "        prediction = (probabilities >= threshold).astype(int)\n",
    "        prob_dict = dict(zip(mlb.classes_, probabilities[0]))\n",
    "    else:\n",
    "        prediction = model.predict(processed)\n",
    "        prob_dict = None\n",
    "    \n",
    "    labels = mlb.inverse_transform(prediction)\n",
    "    \n",
    "    print(\"\\nPrediction Results:\")\n",
    "    print(f\"Input text: {text[:100]}...\")\n",
    "    print(f\"Predicted genres: {', '.join(labels[0]) or 'None'}\")\n",
    "    \n",
    "    if prob_dict:\n",
    "        print(\"\\nClass Probabilities:\")\n",
    "        for genre, prob in sorted(prob_dict.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"{genre:<25} {prob:.2%}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load latest model\n",
    "    model, preprocessor, mlb = load_resources()\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluate_model(model, preprocessor, mlb)\n",
    "    \n",
    "    # Make prediction\n",
    "    sample_text = \"A space opera featuring interstellar politics, alien species, and epic battles between galactic empires\"\n",
    "    predict_genre(model, preprocessor, mlb, sample_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
