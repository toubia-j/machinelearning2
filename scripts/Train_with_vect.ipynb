{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ1t-j0JQE_Z",
        "outputId": "d5726ebe-798a-4821-8eae-e214151b0443",
        "colab": {
          "referenced_widgets": [
            "b8d99d64251f402eac58830b2ad3ff95",
            "2fdd982494dc474c8390960b650df1f2",
            "6fafedb468204e609bd82fbc6c518e25",
            "cd24e0fad62547dabfd2c06e85057092",
            "2c4ad84688e54bc28c0dd1d1c4a16b4a",
            "4a04e44c26c34353a6470af36b94e1d5",
            "968eb8aadab44ec499c0933e19250440",
            "1fd5a262d22c414b854d60475a67f2db",
            "c2d16684924249ab9604a04342bc9419",
            "ad6611024a2641c1bbc767af66920fa5",
            "5663f945595b467c8dcb29c4ed12e516"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\code\\python\\python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "Training model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\yannv\\AppData\\Local\\Temp\\ipykernel_36472\\659375559.py:17: DtypeWarning: Columns (1,3,4,6,7,8,9,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047,2048,2049,2050,2051,2052,2053,2054,2055,2056,2057,2058,2059,2060,2061,2062,2063,2064,2065,2066,2067,2068,2069,2070,2071,2072,2073,2074,2075,2076,2077,2078,2079,2080,2081,2082,2083,2084,2085,2086,2087,2088,2089,2090,2091,2092,2093,2094,2095,2096,2097,2098,2099,2100,2101,2102,2103,2104,2105,2106,2107,2108,2109,2110,2111,2112,2113,2114,2115,2116,2117,2118,2119,2120,2121,2122,2123,2124,2125,2126,2127,2128,2129,2130,2131,2132,2133,2134,2135,2136,2137,2138,2139,2140,2141,2142,2143,2144,2145,2146,2147,2148,2149,2150,2151,2152,2153,2154,2155,2156,2157,2158,2159,2160,2161,2162,2163,2164,2165,2166,2167,2168,2169,2170,2171,2172,2173,2174,2175,2176,2177,2178,2179,2180,2181,2182,2183,2184,2185,2186,2187,2188,2189,2190,2191,2192,2193,2194,2195,2196,2197,2198,2199,2200,2201,2202,2203,2204,2205,2206,2207,2208,2209,2210,2211,2212,2213,2214,2215,2216,2217,2218,2219,2220,2221,2222,2223,2224,2225,2226,2227,2228,2229,2230,2231,2232,2233,2234,2235,2236,2237,2238,2239,2240,2241,2242,2243,2244,2245,2246,2247,2248,2249,2250,2251,2252,2253,2254,2255,2256,2257,2258,2259,2260,2261,2262,2263,2264,2265,2266,2267,2268,2269,2270,2271,2272,2273,2274,2275,2276,2277,2278,2279,2280,2281,2282,2283,2284,2285,2286,2287,2288,2289,2290,2291,2292,2293,2294,2295,2296,2297,2298,2299,2300,2301,2302,2303,2304,2305,2306,2307,2308,2309,2310,2311,2312,2313,2314,2315,2316,2317,2318,2319,2320,2321,2322,2323,2324,2325,2326,2327,2328,2329,2330,2331,2332,2333,2334,2335,2336,2337,2338,2339,2340,2341,2342,2343,2344,2345,2346,2347,2348,2349,2350,2351,2352,2353,2354,2355,2356,2357,2358,2359,2360,2361,2362,2363,2364,2365,2366,2367,2368,2369,2370,2371,2372,2373,2374,2375,2376,2377,2378,2379,2380,2381,2382,2383,2384,2385,2386,2387,2388,2389,2390,2391,2392,2393,2394,2395,2396,2397,2398,2399,2400,2401,2402,2403,2404,2405,2406,2407,2408,2409,2410,2411,2412,2413,2414,2415,2416,2417,2418,2419,2420,2421,2422,2423,2424,2425,2426,2427,2428,2429,2430,2431,2432,2433,2434,2435,2436,2437,2438,2439,2440,2441,2442,2443,2444,2445,2446,2447,2448,2449,2450,2451,2452,2453,2454,2455,2456,2457,2458,2459,2460,2461,2462,2463,2464,2465,2466,2467,2468,2469,2470,2471,2472,2473,2474,2475,2476,2477,2478,2479,2480,2481,2482,2483,2484,2485,2486,2487,2488,2489,2490,2491,2492,2493,2494,2495,2496,2497,2498,2499,2500,2501,2502,2503,2504,2505,2506,2507,2508,2509,2510,2511,2512,2513,2514,2515,2516,2517,2518,2519,2520,2521,2522,2523,2524,2525,2526,2527,2528,2529,2530,2531,2532,2533,2534,2535,2536,2537,2538,2539,2540,2541,2542,2543,2544,2545,2546,2547,2548,2549,2550,2551,2552,2553,2554,2555,2556,2557,2558,2559,2560,2561,2562,2563,2564,2565,2566,2567,2568,2569,2570,2571,2572,2573,2574,2575,2576,2577,2578,2579,2580,2581,2582,2583,2584,2585,2586,2587,2588,2589,2590,2591,2592,2593,2594,2595,2596,2597,2598,2599,2600,2601,2602,2603,2604,2605,2606,2607,2608,2609,2610,2611,2612,2613,2614,2615,2616,2617,2618,2619,2620,2621,2622,2623,2624,2625,2626,2627,2628,2629,2630,2631,2632,2633,2634,2635,2636,2637,2638,2639,2640,2641,2642,2643,2644,2645,2646,2647,2648,2649,2650,2651,2652,2653,2654,2655,2656,2657,2658,2659,2660,2661,2662,2663,2664,2665,2666,2667,2668,2669,2670,2671,2672,2673,2674,2675,2676,2677,2678,2679,2680,2681,2682,2683,2684,2685,2686,2687,2688,2689,2690,2691,2692,2693,2694,2695,2696,2697,2698,2699,2700,2701,2702,2703,2704,2705,2706,2707,2708,2709,2710,2711,2712,2713,2714,2715,2716,2717,2718,2719,2720,2721,2722,2723,2724,2725,2726,2727,2728,2729,2730,2731,2732,2733,2734,2735,2736,2737,2738,2739,2740,2741,2742,2743,2744,2745,2746,2747,2748,2749,2750,2751,2752,2753,2754,2755,2756,2757,2758,2759,2760,2761,2762,2763,2764,2765,2766,2767,2768,2769,2770,2771,2772,2773,2774,2775,2776,2777,2778,2779,2780,2781,2782,2783,2784,2785,2786,2787,2788,2789,2790,2791,2792,2793,2794,2795,2796,2797,2798,2799,2800,2801,2802,2803,2804,2805,2806,2807,2808,2809,2810,2811,2812,2813,2814,2815,2816,2817,2818,2819,2820,2821,2822,2823,2824,2825,2826,2827,2828,2829,2830,2831,2832,2833,2834,2835,2836,2837,2838,2839,2840,2841,2842,2843,2844,2845,2846,2847,2848,2849,2850,2851,2852,2853,2854,2855,2856,2857,2858,2859,2860,2861,2862,2863,2864,2865,2866,2867,2868,2869,2870,2871,2872,2873,2874,2875,2876,2877,2878,2879,2880,2881,2882,2883,2884,2885,2886,2887,2888,2889,2890,2891,2892,2893,2894,2895,2896,2897,2898,2899,2900,2901,2902,2903,2904,2905,2906,2907,2908,2909,2910,2911,2912,2913,2914,2915,2916,2917,2918,2919,2920,2921,2922,2923,2924,2925,2926,2927,2928,2929,2930,2931,2932,2933,2934,2935,2936,2937,2938,2939,2940,2941,2942,2943,2944,2945,2946,2947,2948,2949,2950,2951,2952,2953,2954,2955,2956,2957,2958,2959,2960,2961,2962,2963,2964,2965,2966,2967,2968,2969,2970,2971,2972,2973,2974,2975,2976,2977,2978,2979,2980,2981,2982,2983,2984,2985,2986,2987,2988,2989,2990,2991,2992,2993,2994,2995,2996,2997,2998,2999,3000,3001,3002,3003,3004,3005,3006,3007,3008,3009,3010,3011,3012,3013,3014,3015,3016,3017,3018,3019,3020,3021,3022,3023,3024,3025,3026,3027,3028,3029,3030,3031,3032,3033,3034,3035,3036,3037,3038,3039,3040,3041,3042,3043,3044,3045,3046,3047,3048,3049,3050,3051,3052,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3073,3074,3075,3076,3077,3078,3079,3080,3081,3082,3083,3084,3085,3086,3087,3088,3089,3090,3091,3092,3093,3094,3095,3096,3097,3098,3099,3100,3101,3102,3103,3104,3105,3106,3107,3108,3109,3110,3111,3112,3113,3114,3115,3116,3117,3118,3119,3120,3121,3122,3123,3124,3125,3126,3127,3128,3129,3130,3131,3132,3133,3134,3135,3136,3137,3138,3139,3140,3141,3142,3143,3144,3145,3146,3147,3148,3149,3150,3151,3152,3153,3154,3155,3156,3157,3158,3159,3160,3161,3162,3163,3164,3165,3166,3167,3168,3169,3170,3171,3172,3173,3174,3175,3176,3177,3178,3179,3180,3181,3182,3183,3184,3185,3186,3187,3188,3189,3190,3191,3192,3193,3194,3195,3196,3197,3198,3199,3200,3201,3202,3203,3204,3205,3206,3207,3208,3209,3210,3211,3212,3213,3214,3215,3216,3217,3218,3219,3220,3221,3222,3223,3224,3225,3226,3227,3228,3229,3230,3231,3232,3233,3234,3235,3236,3237,3238,3239,3240,3241,3242,3243,3244,3245,3246,3247,3248,3249,3250,3251,3252,3253,3254,3255,3256,3257,3258,3259,3260,3261,3262,3263,3264,3265,3266,3267,3268,3269,3270,3271,3272,3273,3274,3275,3276,3277,3278,3279,3280,3281,3282,3283,3284,3285,3286,3287,3288,3289,3290,3291,3292,3293,3294,3295,3296,3297,3298,3299,3300,3301,3302,3303,3304,3305,3306,3307,3308,3309,3310,3311,3312,3313,3314,3315,3316,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3330,3331,3332,3333,3334,3335,3336,3337,3338,3339,3340,3341,3342,3343,3344,3345,3346,3347,3348,3349,3350,3351,3352,3353,3354,3355,3356,3357,3358,3359,3360,3361,3362,3363,3364,3365,3366,3367,3368,3369,3370,3371,3372,3373,3374,3375,3376,3377,3378,3379,3380,3381,3382,3383,3384,3385,3386,3387,3388,3389,3390,3391,3392,3393,3394,3395,3396,3397,3398,3399,3400,3401,3402,3403,3404,3405,3406,3407,3408,3409,3410,3411,3412,3413,3414,3415,3416,3417,3418,3419,3420,3421,3422,3423,3424,3425,3426,3427,3428,3429,3430,3431,3432,3433,3434,3435,3436,3437,3438,3439,3440,3441,3442,3443,3444,3445,3446,3447,3448,3449,3450,3451,3452,3453,3454,3455,3456,3457,3458,3459,3460,3461,3462,3463,3464,3465,3466,3467,3468,3469,3470,3471,3472,3473,3474,3475,3476,3477,3478,3479,3480,3481,3482,3483,3484,3485,3486,3487,3488,3489,3490,3491,3492,3493,3494,3495,3496,3497,3498,3499,3500,3501,3502,3503,3504,3505,3506,3507,3508,3509,3510,3511,3512,3513,3514,3515,3516,3517,3518,3519,3520,3521,3522,3523,3524,3525,3526,3527,3528,3529,3530,3531,3532,3533,3534,3535,3536,3537,3538,3539,3540,3541,3542,3543,3544,3545,3546,3547,3548,3549,3550,3551,3552,3553,3554,3555,3556,3557,3558,3559,3560,3561,3562,3563,3564,3565,3566,3567,3568,3569,3570,3571,3572,3573,3574,3575,3576,3577,3578,3579,3580,3581,3582,3583,3584,3585,3586,3587,3588,3589,3590,3591,3592,3593,3594,3595,3596,3597,3598,3599,3600,3601,3602,3603,3604,3605,3606,3607,3608,3609,3610,3611,3612,3613,3614,3615,3616,3617,3618,3619,3620,3621,3622,3623,3624,3625,3626,3627,3628,3629,3630,3631,3632,3633,3634,3635,3636,3637,3638,3639,3640,3641,3642,3643,3644,3645,3646,3647,3648,3649,3650,3651,3652,3653,3654,3655,3656,3657,3658,3659,3660,3661,3662,3663,3664,3665,3666,3667,3668,3669,3670,3671,3672,3673,3674,3675,3676,3677,3678,3679,3680,3681,3682,3683,3684,3685,3686,3687,3688,3689,3690,3691,3692,3693,3694,3695,3696,3697,3698,3699,3700,3701,3702,3703,3704,3705,3706,3707,3708,3709,3710,3711,3712,3713,3714,3715,3716,3717,3718,3719,3720,3721,3722,3723,3724,3725,3726,3727,3728,3729,3730,3731,3732,3733,3734,3735,3736,3737,3738,3739,3740,3741,3742,3743,3744,3745,3746,3747,3748,3749,3750,3751,3752,3753,3754,3755,3756,3757,3758,3759,3760,3761,3762,3763,3764,3765,3766,3767,3768,3769,3770,3771,3772,3773,3774,3775,3776,3777,3778,3779,3780,3781,3782,3783) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df2 = pd.read_csv(file_path2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        20\n",
            "           1       0.00      0.00      0.00         6\n",
            "           2       0.00      0.00      0.00         2\n",
            "           4       0.00      0.00      0.00         2\n",
            "           5       0.00      0.00      0.00         3\n",
            "           6       0.00      0.00      0.00        63\n",
            "           7       0.00      0.00      0.00       251\n",
            "           8       0.00      0.00      0.00         2\n",
            "          10       0.00      0.00      0.00        11\n",
            "          11       0.00      0.00      0.00         3\n",
            "          12       0.00      0.00      0.00        29\n",
            "          13       0.00      0.00      0.00         1\n",
            "          14       0.00      0.00      0.00        21\n",
            "          15       0.00      0.00      0.00        21\n",
            "          16       0.00      0.00      0.00         2\n",
            "          17       0.00      0.00      0.00        53\n",
            "          18       0.00      0.00      0.00         8\n",
            "          19       0.00      0.00      0.00        21\n",
            "          20       0.00      0.00      0.00        31\n",
            "          21       0.00      0.00      0.00         7\n",
            "          22       0.00      0.00      0.00        34\n",
            "          23       0.00      0.00      0.00         1\n",
            "          24       0.00      0.00      0.00         1\n",
            "          25       0.00      0.00      0.00       136\n",
            "          26       0.00      0.00      0.00         9\n",
            "          27       0.00      0.00      0.00         1\n",
            "          28       0.00      0.00      0.00         4\n",
            "          29       0.00      0.00      0.00        81\n",
            "          30       0.00      0.00      0.00        25\n",
            "          31       0.00      0.00      0.00         5\n",
            "          32       0.00      0.00      0.00         5\n",
            "          33       0.00      0.00      0.00         6\n",
            "          34       0.00      0.00      0.00         1\n",
            "          37       0.00      0.00      0.00         5\n",
            "          38       0.00      0.00      0.00        11\n",
            "          39       0.00      0.00      0.00        22\n",
            "          40       0.00      0.00      0.00         1\n",
            "          41       0.00      0.00      0.00         1\n",
            "          42       0.00      0.00      0.00         2\n",
            "          43       0.00      0.00      0.00         1\n",
            "          44       0.00      0.00      0.00         2\n",
            "          45       0.00      0.00      0.00        23\n",
            "          46       0.00      0.00      0.00        30\n",
            "          47       0.00      0.00      0.00         1\n",
            "          48       0.00      0.00      0.00        15\n",
            "          50       0.00      0.00      0.00         4\n",
            "          55       0.00      0.00      0.00        24\n",
            "          57       0.00      0.00      0.00         1\n",
            "          58       0.00      0.00      0.00        21\n",
            "          59       0.00      0.00      0.00         2\n",
            "          60       0.00      0.00      0.00         2\n",
            "          61       0.00      0.00      0.00         2\n",
            "          62       0.00      0.00      0.00        14\n",
            "          63       0.00      0.00      0.00         1\n",
            "          64       0.00      0.00      0.00         2\n",
            "          65       0.00      0.00      0.00        12\n",
            "          66       0.00      0.00      0.00       411\n",
            "          67       0.00      0.00      0.00         2\n",
            "          68       0.00      0.00      0.00        10\n",
            "          69       0.00      0.00      0.00         5\n",
            "          70       0.00      0.00      0.00         8\n",
            "          72       0.00      0.00      0.00         1\n",
            "          74       0.00      0.00      0.00        52\n",
            "          75       0.00      0.00      0.00        10\n",
            "          76       0.00      0.00      0.00         2\n",
            "          77       0.00      0.00      0.00       161\n",
            "          78       0.00      0.00      0.00        70\n",
            "          80       0.00      0.00      0.00        37\n",
            "          81       0.00      0.00      0.00        14\n",
            "          82       0.00      0.00      0.00         3\n",
            "          83       0.00      0.00      0.00        39\n",
            "          84       0.00      0.00      0.00         1\n",
            "          88       0.00      0.00      0.00        47\n",
            "          89       0.00      0.00      0.00         8\n",
            "          90       0.00      0.00      0.00        46\n",
            "          91       0.00      0.00      0.00         1\n",
            "          92       0.00      0.00      0.00         1\n",
            "          93       0.00      0.00      0.00         4\n",
            "          94       0.00      0.00      0.00         1\n",
            "          95       0.00      0.00      0.00         2\n",
            "          96       0.00      0.00      0.00        30\n",
            "          97       0.00      0.00      0.00        20\n",
            "          98       0.00      0.00      0.00         5\n",
            "          99       0.00      0.00      0.00         4\n",
            "         100       0.00      0.00      0.00         6\n",
            "         102       0.00      0.00      0.00         1\n",
            "         103       0.00      0.00      0.00         8\n",
            "         104       0.00      0.00      0.00        20\n",
            "         106       0.00      0.00      0.00        75\n",
            "         108       0.00      0.00      0.00        93\n",
            "         109       0.00      0.00      0.00        68\n",
            "         110       0.00      0.00      0.00         6\n",
            "         111       0.00      0.00      0.00         2\n",
            "         112       0.00      0.00      0.00        63\n",
            "         114       0.00      0.00      0.00        15\n",
            "         115       0.00      0.00      0.00        12\n",
            "         116       0.00      0.00      0.00         3\n",
            "         117       0.00      0.00      0.00        66\n",
            "         118       0.00      0.00      0.00         6\n",
            "         119       0.00      0.00      0.00         3\n",
            "         120       0.00      0.00      0.00         3\n",
            "         121       0.00      0.00      0.00       119\n",
            "         123       0.00      0.00      0.00        12\n",
            "         124       0.00      0.00      0.00        51\n",
            "         125       0.00      0.00      0.00         2\n",
            "         126       0.00      0.00      0.00        35\n",
            "         127       0.00      0.00      0.00         4\n",
            "         129       0.00      0.00      0.00        39\n",
            "         130       0.00      0.00      0.00         1\n",
            "         132       0.00      0.00      0.00         4\n",
            "         134       0.00      0.00      0.00        35\n",
            "         135       0.00      0.00      0.00        15\n",
            "         136       0.00      0.00      0.00         4\n",
            "         138       0.00      0.00      0.00         8\n",
            "         140       0.00      0.00      0.00         1\n",
            "         141       0.00      0.00      0.00         4\n",
            "         142       0.00      0.00      0.00         2\n",
            "         143       0.00      0.00      0.00        24\n",
            "         144       0.00      0.00      0.00        97\n",
            "         145       0.00      0.00      0.00        41\n",
            "         147       0.00      0.00      0.00        14\n",
            "         148       0.00      0.00      0.00        60\n",
            "         149       0.00      0.00      0.00        25\n",
            "         150       0.00      0.00      0.00         3\n",
            "         151       0.00      0.00      0.00         6\n",
            "         152       0.00      0.00      0.00         1\n",
            "         153       0.00      0.00      0.00        36\n",
            "         154       0.00      0.00      0.00        24\n",
            "         155       0.00      0.00      0.00        31\n",
            "         156       0.00      0.00      0.00         9\n",
            "         157       0.00      0.00      0.00        15\n",
            "         158       0.00      0.00      0.00        87\n",
            "         159       0.00      0.00      0.00        21\n",
            "         161       0.00      0.00      0.00         4\n",
            "         162       0.00      0.00      0.00         3\n",
            "         163       0.00      0.00      0.00        57\n",
            "         164       0.00      0.00      0.00       135\n",
            "         165       0.00      0.00      0.00         1\n",
            "         166       0.00      0.00      0.00         6\n",
            "         167       0.00      0.00      0.00         5\n",
            "         168       0.00      0.00      0.00         4\n",
            "         169       0.00      0.00      0.00        29\n",
            "         170       0.00      0.00      0.00         3\n",
            "         171       0.00      0.00      0.00       150\n",
            "         173       0.00      0.00      0.00        10\n",
            "         174       0.00      0.00      0.00         8\n",
            "         175       0.00      0.00      0.00        12\n",
            "         176       0.00      0.00      0.00        16\n",
            "         177       0.00      0.00      0.00         1\n",
            "         178       0.00      0.00      0.00         6\n",
            "         179       0.00      0.00      0.00        24\n",
            "         180       0.00      0.00      0.00        13\n",
            "         183       0.00      0.00      0.00        15\n",
            "         184       0.00      0.00      0.00        21\n",
            "         185       0.00      0.00      0.00        13\n",
            "         186       0.00      0.00      0.00        41\n",
            "         187       0.00      0.00      0.00        47\n",
            "         188       0.00      0.00      0.00         2\n",
            "         189       0.00      0.00      0.00        35\n",
            "         191       0.00      0.00      0.00         6\n",
            "         192       0.00      0.00      0.00         5\n",
            "         193       0.63      1.00      0.77     12737\n",
            "         194       0.00      0.00      0.00        30\n",
            "         196       0.00      0.00      0.00         5\n",
            "         198       0.00      0.00      0.00         4\n",
            "         199       0.00      0.00      0.00         1\n",
            "         200       0.00      0.00      0.00         2\n",
            "         202       0.00      0.00      0.00         3\n",
            "         204       0.00      0.00      0.00         1\n",
            "         205       0.00      0.00      0.00        13\n",
            "         206       0.00      0.00      0.00        58\n",
            "         208       0.00      0.00      0.00       191\n",
            "         210       0.00      0.00      0.00        22\n",
            "         211       0.00      0.00      0.00        11\n",
            "         212       0.00      0.00      0.00         2\n",
            "         214       0.00      0.00      0.00         1\n",
            "         215       0.00      0.00      0.00         2\n",
            "         216       0.00      0.00      0.00         2\n",
            "         217       0.00      0.00      0.00         5\n",
            "         218       0.00      0.00      0.00        16\n",
            "         219       0.00      0.00      0.00        20\n",
            "         221       0.00      0.00      0.00         4\n",
            "         222       0.00      0.00      0.00        10\n",
            "         223       0.00      0.00      0.00        18\n",
            "         224       0.00      0.00      0.00        24\n",
            "         225       0.00      0.00      0.00         1\n",
            "         226       0.00      0.00      0.00         1\n",
            "         227       0.00      0.00      0.00      1204\n",
            "         228       0.00      0.00      0.00        98\n",
            "         229       0.00      0.00      0.00        12\n",
            "         230       0.00      0.00      0.00        23\n",
            "         231       0.00      0.00      0.00        50\n",
            "         232       0.00      0.00      0.00         2\n",
            "         233       0.00      0.00      0.00        15\n",
            "         234       0.00      0.00      0.00         2\n",
            "         235       0.00      0.00      0.00        30\n",
            "         236       0.00      0.00      0.00        85\n",
            "         237       0.00      0.00      0.00         3\n",
            "         238       0.00      0.00      0.00        11\n",
            "         239       0.00      0.00      0.00       117\n",
            "         240       0.00      0.00      0.00         2\n",
            "         241       0.00      0.00      0.00         7\n",
            "         243       0.00      0.00      0.00        16\n",
            "         244       0.00      0.00      0.00         1\n",
            "         247       0.00      0.00      0.00        13\n",
            "         249       0.00      0.00      0.00         2\n",
            "         250       0.00      0.00      0.00        73\n",
            "         251       0.00      0.00      0.00        13\n",
            "         253       0.00      0.00      0.00        21\n",
            "         254       0.00      0.00      0.00         1\n",
            "         255       0.00      0.00      0.00         4\n",
            "         256       0.00      0.00      0.00        36\n",
            "         257       0.00      0.00      0.00         3\n",
            "         259       0.00      0.00      0.00         7\n",
            "         260       0.00      0.00      0.00        38\n",
            "         261       0.00      0.00      0.00        11\n",
            "         262       0.00      0.00      0.00         1\n",
            "         263       0.00      0.00      0.00         5\n",
            "         264       0.00      0.00      0.00        10\n",
            "         266       0.00      0.00      0.00         8\n",
            "         267       0.00      0.00      0.00       161\n",
            "         268       0.00      0.00      0.00         4\n",
            "         269       0.00      0.00      0.00         6\n",
            "         270       0.00      0.00      0.00        29\n",
            "         271       0.00      0.00      0.00        11\n",
            "         272       0.00      0.00      0.00         8\n",
            "         275       0.00      0.00      0.00       122\n",
            "         276       0.00      0.00      0.00         3\n",
            "         277       0.00      0.00      0.00         7\n",
            "         278       0.00      0.00      0.00         6\n",
            "         279       0.00      0.00      0.00        12\n",
            "         281       0.00      0.00      0.00        29\n",
            "         283       0.00      0.00      0.00        26\n",
            "         284       0.00      0.00      0.00         4\n",
            "         285       0.00      0.00      0.00       114\n",
            "         286       0.00      0.00      0.00        18\n",
            "         287       0.00      0.00      0.00        31\n",
            "         288       0.00      0.00      0.00        14\n",
            "         289       0.00      0.00      0.00         1\n",
            "         290       0.00      0.00      0.00         5\n",
            "         291       0.00      0.00      0.00         3\n",
            "         292       0.00      0.00      0.00         1\n",
            "         293       0.00      0.00      0.00        28\n",
            "         294       0.00      0.00      0.00         2\n",
            "         295       0.00      0.00      0.00         2\n",
            "         296       0.00      0.00      0.00         1\n",
            "         298       0.00      0.00      0.00        51\n",
            "         299       0.00      0.00      0.00         1\n",
            "         300       0.00      0.00      0.00        32\n",
            "         301       0.00      0.00      0.00         8\n",
            "         302       0.00      0.00      0.00         1\n",
            "         305       0.00      0.00      0.00        56\n",
            "         306       0.00      0.00      0.00        77\n",
            "         307       0.00      0.00      0.00         4\n",
            "         308       0.00      0.00      0.00         1\n",
            "         310       0.00      0.00      0.00        17\n",
            "         311       0.00      0.00      0.00        18\n",
            "         312       0.00      0.00      0.00        21\n",
            "         313       0.00      0.00      0.00         4\n",
            "         314       0.00      0.00      0.00       230\n",
            "\n",
            "    accuracy                           0.63     20289\n",
            "   macro avg       0.00      0.00      0.00     20289\n",
            "weighted avg       0.39      0.63      0.48     20289\n",
            "\n",
            "\n",
            "Model saved to: ../models\\genre_predictor_model.joblib\n",
            "Label encoder saved to: ../models\\label_encoder.joblib\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\code\\python\\python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\code\\python\\python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\code\\python\\python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8d99d64251f402eac58830b2ad3ff95",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\code\\python\\python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yannv\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2fdd982494dc474c8390960b650df1f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fafedb468204e609bd82fbc6c518e25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd24e0fad62547dabfd2c06e85057092",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c4ad84688e54bc28c0dd1d1c4a16b4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a04e44c26c34353a6470af36b94e1d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "968eb8aadab44ec499c0933e19250440",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fd5a262d22c414b854d60475a67f2db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2d16684924249ab9604a04342bc9419",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad6611024a2641c1bbc767af66920fa5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\code\\python\\python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5663f945595b467c8dcb29c4ed12e516",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prediction Results:\n",
            "Primary predicted genre: fiction\n",
            "\n",
            "Top 3 genre predictions with probabilities:\n",
            "female friendship: 62.25%\n",
            "indic fiction (english): 6.17%\n",
            "biography & autobiography: 2.45%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\code\\python\\python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "def load_and_prepare_data(file_path1, file_path2):\n",
        "    \"\"\"\n",
        "    Load and combine the two parts of the dataset\n",
        "    \"\"\"\n",
        "    # Load both parts of the data\n",
        "    df1 = pd.read_csv(file_path1)\n",
        "    df2 = pd.read_csv(file_path2)\n",
        "\n",
        "    # Combine the dataframes\n",
        "    df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
        "\n",
        "    # Get the embedding columns (they start with 'desc_embed_')\n",
        "    embedding_cols = [col for col in df.columns if col.startswith('desc_embed_')]\n",
        "\n",
        "    # Extract features (embeddings) and target (categories)\n",
        "    X = df[embedding_cols].values\n",
        "    y = df['categories']\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def train_genre_predictor(model_save_path='../models'):\n",
        "    \"\"\"\n",
        "    Train a model to predict book genres based on synopsis embeddings and save it\n",
        "\n",
        "    Args:\n",
        "        model_save_path (str): Directory to save the model files\n",
        "\n",
        "    Returns:\n",
        "        tuple: Trained model and label encoder\n",
        "    \"\"\"\n",
        "    # Create models directory if it doesn't exist\n",
        "    os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "    # Load and prepare the data\n",
        "    X, y = load_and_prepare_data('../data/final_data_part1.csv', '../data/final_data_part2.csv')\n",
        "\n",
        "    # Encode the genre labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y_encoded, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"\\nModel Performance:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Save the model and label encoder\n",
        "    model_file = os.path.join(model_save_path, 'genre_predictor_model.joblib')\n",
        "    encoder_file = os.path.join(model_save_path, 'label_encoder.joblib')\n",
        "\n",
        "    joblib.dump(model, model_file)\n",
        "    joblib.dump(label_encoder, encoder_file)\n",
        "\n",
        "    print(f\"\\nModel saved to: {model_file}\")\n",
        "    print(f\"Label encoder saved to: {encoder_file}\")\n",
        "\n",
        "    return model, label_encoder\n",
        "\n",
        "def load_genre_predictor(model_save_path='../models'):\n",
        "    \"\"\"\n",
        "    Load the trained model and label encoder from files\n",
        "\n",
        "    Args:\n",
        "        model_save_path (str): Directory containing the model files\n",
        "\n",
        "    Returns:\n",
        "        tuple: Loaded model and label encoder\n",
        "    \"\"\"\n",
        "    model_file = os.path.join(model_save_path, 'genre_predictor_model.joblib')\n",
        "    encoder_file = os.path.join(model_save_path, 'label_encoder.joblib')\n",
        "\n",
        "    if not (os.path.exists(model_file) and os.path.exists(encoder_file)):\n",
        "        raise FileNotFoundError(\"Model files not found. Please train the model first.\")\n",
        "\n",
        "    model = joblib.load(model_file)\n",
        "    label_encoder = joblib.load(encoder_file)\n",
        "\n",
        "    return model, label_encoder\n",
        "\n",
        "def predict_genre(model, label_encoder, synopsis, sentence_transformer):\n",
        "    \"\"\"\n",
        "    Predict the genre of a new book based on its synopsis\n",
        "    \"\"\"\n",
        "    # Generate embedding for the new synopsis\n",
        "    synopsis_embedding = sentence_transformer.encode([synopsis])\n",
        "\n",
        "    # Make prediction\n",
        "    genre_encoded = model.predict(synopsis_embedding)\n",
        "    genre = label_encoder.inverse_transform(genre_encoded)\n",
        "\n",
        "    # Get prediction probabilities\n",
        "    proba = model.predict_proba(synopsis_embedding)\n",
        "    top_genres_idx = np.argsort(proba[0])[-3:][::-1]  # Get top 3 genres\n",
        "    top_genres = label_encoder.inverse_transform(top_genres_idx)\n",
        "    top_probas = proba[0][top_genres_idx]\n",
        "\n",
        "    return genre[0], list(zip(top_genres, top_probas))\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Train and save the model\n",
        "    print(\"Training model...\")\n",
        "    model, label_encoder = train_genre_predictor()\n",
        "\n",
        "    # Alternative: Load previously trained model\n",
        "    # model, label_encoder = load_genre_predictor()\n",
        "\n",
        "    # Load the sentence transformer\n",
        "    sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    # Example prediction\n",
        "    test_synopsis = \"\"\"\n",
        "    In a world where magic is forbidden, a young girl discovers she has\n",
        "    extraordinary powers. She must learn to control her abilities while\n",
        "    hiding from those who would persecute her for her gifts.\n",
        "    \"\"\"\n",
        "\n",
        "    predicted_genre, top_predictions = predict_genre(\n",
        "        model, label_encoder, test_synopsis, sentence_transformer\n",
        "    )\n",
        "\n",
        "    print(\"\\nPrediction Results:\")\n",
        "    print(f\"Primary predicted genre: {predicted_genre}\")\n",
        "    print(\"\\nTop 3 genre predictions with probabilities:\")\n",
        "    for genre, probability in top_predictions:\n",
        "        print(f\"{genre}: {probability:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsdFNZGNQE_i",
        "outputId": "82a03824-0838-4144-f31c-3d1cdacf1abf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\code\\python\\python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model, label_encoder = load_genre_predictor('../models')\n",
        "sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "test_synopsis = \"\"\"\n",
        "In a world where magic is forbidden\n",
        "Chris is a young lad who has always been fascinated by magic. He has always wanted to be a magician but his parents have always discouraged him from doing so. They believe that magic is evil and that it is not something that should be practiced. Chris has always been\n",
        "\"\"\"\n",
        "predicted_genre, top_predictions = predict_genre(model, label_encoder, test_synopsis, sentence_transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKwL77nXQE_j",
        "outputId": "4ca22505-b7f0-4913-819e-2085ef5f3c34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'fiction'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_genre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72MBHh4TQE_j",
        "outputId": "368fe717-103d-4a2b-b649-acd7b33cf40c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('female friendship', 0.6255455368019699),\n",
              " ('indic fiction (english)', 0.05977310694324284),\n",
              " ('biography & autobiography', 0.021851509435945705)]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "top_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOTk4ibwVJzz"
      },
      "source": [
        "**Modele XGBoostClassifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP8a2BgrVM-9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTRWkUAgekgJ",
        "outputId": "5cd89d1f-7b65-49e9-b3e1-a15a0268f17c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in c:\\code\\python\\python312\\lib\\site-packages (2.1.3)\n",
            "Requirement already satisfied: numpy in c:\\code\\python\\python312\\lib\\site-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: scipy in c:\\code\\python\\python312\\lib\\site-packages (from xgboost) (1.12.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~atplotlib (C:\\code\\python\\python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~atplotlib (C:\\code\\python\\python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~atplotlib (C:\\code\\python\\python312\\Lib\\site-packages)\n",
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 25.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence_transformers in c:\\code\\python\\python312\\lib\\site-packages (3.4.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\code\\python\\python312\\lib\\site-packages (from sentence_transformers) (4.44.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\yannv\\appdata\\roaming\\python\\python312\\site-packages (from sentence_transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\code\\python\\python312\\lib\\site-packages (from sentence_transformers) (2.4.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in c:\\code\\python\\python312\\lib\\site-packages (from sentence_transformers) (1.4.0)\n",
            "Requirement already satisfied: scipy in c:\\code\\python\\python312\\lib\\site-packages (from sentence_transformers) (1.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\code\\python\\python312\\lib\\site-packages (from sentence_transformers) (0.24.5)\n",
            "Requirement already satisfied: Pillow in c:\\code\\python\\python312\\lib\\site-packages (from sentence_transformers) (10.2.0)\n",
            "Requirement already satisfied: filelock in c:\\code\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\code\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\yannv\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\code\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in c:\\users\\yannv\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yannv\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.10.0)\n",
            "Requirement already satisfied: sympy in c:\\code\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\yannv\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\code\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: setuptools in c:\\code\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (72.0.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\yannv\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\code\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\code\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.7.24)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\code\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\code\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\code\\python\\python312\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\code\\python\\python312\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\code\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\code\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\code\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\code\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\code\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\code\\python\\python312\\lib\\site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~atplotlib (C:\\code\\python\\python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~atplotlib (C:\\code\\python\\python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~atplotlib (C:\\code\\python\\python312\\Lib\\site-packages)\n",
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 25.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost\n",
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRix4xXuVUt-"
      },
      "source": [
        "**Fonction : Charger et préparer les données**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsYa120e4z7v"
      },
      "outputs": [],
      "source": [
        "def clean_categories(df):\n",
        "    # Supprimer les valeurs nulles ou NaN\n",
        "    df['categories'] = df['categories'].fillna('Unknown')\n",
        "\n",
        "    # Forcer toutes les valeurs de 'categories' en type string\n",
        "    df['categories'] = df['categories'].astype(str).str.strip()\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQ3Dv5aIVSq-"
      },
      "outputs": [],
      "source": [
        "def load_and_prepare_data(file_path1='../data/final_data_part1.csv', file_path2='../data/final_data_part2.csv'):\n",
        "    df1 = pd.read_csv(file_path1, low_memory=False)\n",
        "    df2 = pd.read_csv(file_path2, low_memory=False)\n",
        "\n",
        "    df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
        "\n",
        "    # Nettoyer la colonne 'categories'\n",
        "    df = clean_categories(df)\n",
        "\n",
        "    print(\"Unique values in 'categories':\", df['categories'].unique())\n",
        "\n",
        "    embedding_cols = [col for col in df.columns if col.startswith('desc_embed_')]\n",
        "    X = df[embedding_cols].values\n",
        "    y = df['categories']\n",
        "\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVo9y6VFVz_q"
      },
      "source": [
        "**Fonction : Entraîner le modèle de prédiction des genres**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L-ey4xCVvzM"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def train_genre_predictor():\n",
        "    # Charger les données\n",
        "    print(\"Loading data...\")\n",
        "    X, y = load_and_prepare_data('../data/final_data_part1.csv', '../data/final_data_part2.csv')\n",
        "\n",
        "    # Encoder les étiquettes\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "    # Supprimer les classes rares\n",
        "    def remove_rare_classes(X, y, min_samples=2):\n",
        "        \"\"\"Supprime les classes ayant moins de min_samples.\"\"\"\n",
        "        class_counts = pd.Series(y).value_counts()\n",
        "        rare_classes = class_counts[class_counts < min_samples].index\n",
        "        mask = ~pd.Series(y).isin(rare_classes)\n",
        "        return X[mask], y[mask]\n",
        "\n",
        "    X, y_encoded = remove_rare_classes(X, y_encoded)\n",
        "\n",
        "    # Réindexer les étiquettes pour les rendre continues\n",
        "    unique_classes = np.unique(y_encoded)\n",
        "    remap_classes = {old: new for new, old in enumerate(unique_classes)}\n",
        "    y_encoded = np.array([remap_classes[label] for label in y_encoded])\n",
        "\n",
        "    # Vérifier les nouvelles classes\n",
        "    print(\"Remaining classes after reindexing:\", np.unique(y_encoded))\n",
        "\n",
        "    # Diviser les données\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        "    )\n",
        "\n",
        "    # Initialiser et entraîner le modèle\n",
        "    model = XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        objective='multi:softmax',\n",
        "        num_class=len(np.unique(y_encoded))  # Correspond désormais au nombre réel de classes\n",
        "    )\n",
        "\n",
        "    print(\"Fitting the model...\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Évaluer les performances\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"Classification report:\\n{classification_report(y_test, y_pred)}\")\n",
        "\n",
        "    return model, label_encoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MLWckudV9EX"
      },
      "source": [
        "**Fonction : Charger le modèle de prédiction des genres**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Kwtg8snWEIr"
      },
      "outputs": [],
      "source": [
        "def load_genre_predictor(model_save_path='../models'):\n",
        "\n",
        "    model_file = os.path.join(model_save_path, 'xgb_genre_predictor_model.pkl')\n",
        "    encoder_file = os.path.join(model_save_path, 'label_encoder.pkl')\n",
        "\n",
        "    if not (os.path.exists(model_file) and os.path.exists(encoder_file)):\n",
        "        raise FileNotFoundError(\"Model files not found. Please train the model first.\")\n",
        "\n",
        "    with open(model_file, 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "    with open(encoder_file, 'rb') as f:\n",
        "        label_encoder = pickle.load(f)\n",
        "\n",
        "    return model, label_encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4cfE0aJWLH0"
      },
      "source": [
        "**Fonction : Prédire le genre d'un livre à partir de son synopsis**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOt3PB0eWJrK"
      },
      "outputs": [],
      "source": [
        "def predict_genre(model, label_encoder, synopsis, sentence_transformer):\n",
        "    \"\"\"\n",
        "    Predict the genre of a new book based on its synopsis\n",
        "    \"\"\"\n",
        "    # Générer l'embedding pour le synopsis donné\n",
        "    synopsis_embedding = sentence_transformer.encode([synopsis])\n",
        "\n",
        "    # Faire la prédiction du genre principal\n",
        "    genre_encoded = model.predict(synopsis_embedding)\n",
        "    genre = label_encoder.inverse_transform(genre_encoded)\n",
        "\n",
        "    # Obtenir les probabilités pour les trois genres les plus probables\n",
        "    proba = model.predict_proba(synopsis_embedding)\n",
        "    top_genres_idx = np.argsort(proba[0])[-3:][::-1]  # Get top 3 genres\n",
        "    top_genres = label_encoder.inverse_transform(top_genres_idx)\n",
        "    top_probas = proba[0][top_genres_idx]\n",
        "\n",
        "    return genre[0], list(zip(top_genres, top_probas))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMacoGT5WVGR"
      },
      "source": [
        "**Programme principale**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "He01PIxg4z7w",
        "outputId": "7c999eb4-ecfe-479a-92f3-7a86de55979d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading and preparing data...\n",
            "Unique values in 'categories': ['fiction' 'english fiction' 'juvenile fiction' 'gambling'\n",
            " 'biography & autobiography' 'animals, mythical' 'england'\n",
            " 'young adult fiction' 'mental illness' 'comics & graphic novels'\n",
            " 'epidemics' \"alzheimer's disease\" 'dangerously mentally ill' 'history'\n",
            " 'horror tales' 'book burning' 'health & fitness' 'china'\n",
            " 'performing arts' 'dent, arthur (fictitious character)'\n",
            " 'american fiction' 'chocolate' 'adultery' 'british' 'allegories'\n",
            " 'education' 'experimental fiction' 'true crime' 'airplane crash survival'\n",
            " 'social science' 'blind' 'arthurian romances' 'provence (france)'\n",
            " 'juvenile nonfiction' 'humor' 'brothers' 'boston (mass.)' 'death'\n",
            " 'ryan, jack (fictitious character)' 'vampires' 'domestic fiction'\n",
            " 'male friendship' 'art' \"children's stories\" 'dystopias' 'drama'\n",
            " 'alienation (social psychology)' 'study aids' 'businessmen' 'religion'\n",
            " 'mentally ill' 'college attendance' 'adventure stories' 'murder'\n",
            " 'escapes' 'castle rock (me. : imaginary place)' 'aging' 'boys' 'bombings'\n",
            " 'cookery' 'poetry' 'london (england)' 'african americans'\n",
            " 'literary criticism' 'literary collections' 'human-animal relationships'\n",
            " 'discworld (imaginary place)' \"children's literature, french\"\n",
            " 'business & economics' 'zero (the number)' 'fantasy fiction'\n",
            " 'mothers and sons' 'cowboys' 'little, stuart (fictitious character)'\n",
            " 'horror stories.' 'crafts & hobbies' 'manuscripts'\n",
            " 'conner, rainie (fictitious character)' 'emotional problems' 'men'\n",
            " 'body, mind & spirit' 'fairy tales' 'demonology'\n",
            " 'detective and mystery stories' 'courtship' 'latin america' 'english'\n",
            " 'classical fiction' 'philosophy' 'authors, english' 'occult fiction'\n",
            " 'atonement' 'christmas' 'divorced people' 'african american men'\n",
            " 'self-help' 'science fiction' 'brewing' 'bullying' 'fantasy'\n",
            " 'fantasy fiction, english' 'black death' 'science' 'criminals'\n",
            " 'life on other planets' 'family & relationships' 'travel' 'americans'\n",
            " 'amish' 'photography' 'american literature' 'language arts & disciplines'\n",
            " 'authors, american' 'angels' 'air pilots' 'science fiction, english'\n",
            " 'computers' 'assassins' 'dracula, count (fictitious character)'\n",
            " 'capitalism' 'beowulf' 'dreams' 'christian life' 'computer programmers'\n",
            " 'political science' 'poets, chilean' 'world war, 1914-1918' 'psychology'\n",
            " 'candy' 'political fiction' 'dominican republic' 'arithmetic'\n",
            " 'family life' 'bible' 'first loves' 'children of the rich' 'accidents'\n",
            " 'aged women' 'divorced women' 'humorous fiction' 'pigeons'\n",
            " 'napoleonic wars, 1800-1815' 'conspiracies' 'elves' 'families' 'authors'\n",
            " 'dallas, eve (fictitious character)' 'handicapped youth'\n",
            " 'baggins, frodo (fictitious character)' 'alcoholics' 'disasters'\n",
            " 'lisbon (portugal)' 'city and town life' 'czech fiction'\n",
            " 'intelligence service' 'friendship' 'farm life' 'college teachers'\n",
            " 'kyoto (japan)' 'college stories' 'nature' 'new york (state)'\n",
            " 'photographers' 'pets' 'espionage' 'surealism' 'girls'\n",
            " 'essentialism (philosophy)' 'organized crime' 'women terrorists'\n",
            " 'igbo (african people)' 'adult education' 'canterbury (england)'\n",
            " 'characters and characteristics in motion pictures' 'abused wives'\n",
            " 'medical' 'english drama' 'sea stories'\n",
            " 'englisch - geschichte - lyrik - aufsatzsammlung' 'los angeles (calif.)'\n",
            " 'american wit and humour' 'belgians' 'reference'\n",
            " 'indic fiction (english)' 'family' 'austria' 'epic literature' 'cancer'\n",
            " 'banks and banking, british' 'adolescence' 'persian gulf war, 1991'\n",
            " 'eretz israel' 'alternative histories (fiction)' 'ballet' 'anger' 'music'\n",
            " 'african american plantation owners' 'cooking' 'business women'\n",
            " \"partition of decedents' estates\" 'clergy' 'black humor (literature)'\n",
            " 'feature films [dvd]' 'fairy tales, english' 'botswana'\n",
            " 'beresford, tommy (fictitious character)' 'benedictine monasteries'\n",
            " 'interplanetary voyages' 'selling' 'cosmology' 'catholic women'\n",
            " 'comic books, strips, etc' 'short stories' 'human cloning'\n",
            " 'african american families' 'literature' 'authors, cuban' 'law'\n",
            " 'female friendship' 'seduction' 'authors, italian' 'humorous stories'\n",
            " 'american poetry' 'divorce' 'christianity' 'war'\n",
            " 'bond, james (fictitious character)' 'slave insurrections' 'apprentices'\n",
            " 'political leadership' 'democracy' 'dublin (ireland)'\n",
            " 'cerebrovascular disease' 'chick lit' 'friendship in adolescence'\n",
            " 'ghost stories, american' 'bereavement' 'mythology, classical'\n",
            " 'conduct of life' 'united states' 'actresses' 'birthparents'\n",
            " 'czech republic' 'identity (psychology)'\n",
            " 'marple, jane (fictitious character)' 'dysfunctional families'\n",
            " 'human-alien encounters' 'battle, superintendent (fictitious character)'\n",
            " 'community life' 'fantasy fiction, american'\n",
            " 'sweet valley (imaginary place)' 'american essays' 'arctic regions'\n",
            " 'australia' 'psycho (motion picture : 1960)' \"children's plays\"\n",
            " \"children's stories, english\" 'babytime resource' 'mormon fundamentalism'\n",
            " 'country life' 'gardening' 'crusades' 'actors' 'prisoners of war'\n",
            " 'archaeological expeditions' 'meditation' 'motion pictures'\n",
            " 'great britain' 'authors, german' 'crime investigations' 'cults' 'canada'\n",
            " 'outlaws' 'azerbaijan' 'african americans in radio broadcasting'\n",
            " 'horror tales, english' 'businesswomen' 'autism'\n",
            " 'drenai (imaginary place)' 'motion picture actors and actresses'\n",
            " 'television' 'good and evil' 'house & home' 'science fiction, american'\n",
            " 'survival' 'buddhism' 'auschwitz (poland : concentration camp)'\n",
            " 'mathematics' 'cornwall (england : county)' 'sports & recreation'\n",
            " 'american drama' 'horror tales, american' 'christmas stories'\n",
            " 'man-woman relationships' 'jews' 'history, modern' 'badgers'\n",
            " 'boarding schools' 'french drama' 'aeneas (legendary character)'\n",
            " 'boarding school-fiction' 'married people'\n",
            " 'continental op (fictitious character)' 'english poetry'\n",
            " 'boats and boating' 'nineteen ninety-eight, a.d.' 'brothers and sisters'\n",
            " 'children of divorced parents' 'social action' 'bus travel' 'games'\n",
            " 'physics' 'bail bond agents' 'historical fiction' 'Unknown']\n",
            "Classes in the dataset: ['Unknown' 'abused wives' 'accidents' 'actors' 'actresses' 'adolescence'\n",
            " 'adult education' 'adultery' 'adventure stories'\n",
            " 'aeneas (legendary character)' 'african american families'\n",
            " 'african american men' 'african american plantation owners'\n",
            " 'african americans' 'african americans in radio broadcasting'\n",
            " 'aged women' 'aging' 'air pilots' 'airplane crash survival' 'alcoholics'\n",
            " 'alienation (social psychology)' 'allegories'\n",
            " 'alternative histories (fiction)' \"alzheimer's disease\" 'american drama'\n",
            " 'american essays' 'american fiction' 'american literature'\n",
            " 'american poetry' 'american wit and humour' 'americans' 'amish' 'angels'\n",
            " 'anger' 'animals, mythical' 'apprentices' 'archaeological expeditions'\n",
            " 'arctic regions' 'arithmetic' 'art' 'arthurian romances' 'assassins'\n",
            " 'atonement' 'auschwitz (poland : concentration camp)' 'australia'\n",
            " 'austria' 'authors' 'authors, american' 'authors, cuban'\n",
            " 'authors, english' 'authors, german' 'authors, italian' 'autism'\n",
            " 'azerbaijan' 'babytime resource' 'badgers'\n",
            " 'baggins, frodo (fictitious character)' 'bail bond agents' 'ballet'\n",
            " 'banks and banking, british'\n",
            " 'battle, superintendent (fictitious character)' 'belgians'\n",
            " 'benedictine monasteries' 'beowulf' 'bereavement'\n",
            " 'beresford, tommy (fictitious character)' 'bible'\n",
            " 'biography & autobiography' 'birthparents' 'black death'\n",
            " 'black humor (literature)' 'blind' 'boarding school-fiction'\n",
            " 'boarding schools' 'boats and boating' 'body, mind & spirit' 'bombings'\n",
            " 'bond, james (fictitious character)' 'book burning' 'boston (mass.)'\n",
            " 'botswana' 'boys' 'brewing' 'british' 'brothers' 'brothers and sisters'\n",
            " 'buddhism' 'bullying' 'bus travel' 'business & economics'\n",
            " 'business women' 'businessmen' 'businesswomen' 'canada' 'cancer' 'candy'\n",
            " 'canterbury (england)' 'capitalism' 'castle rock (me. : imaginary place)'\n",
            " 'catholic women' 'cerebrovascular disease'\n",
            " 'characters and characteristics in motion pictures' 'chick lit'\n",
            " 'children of divorced parents' 'children of the rich'\n",
            " \"children's literature, french\" \"children's plays\" \"children's stories\"\n",
            " \"children's stories, english\" 'china' 'chocolate' 'christian life'\n",
            " 'christianity' 'christmas' 'christmas stories' 'city and town life'\n",
            " 'classical fiction' 'clergy' 'college attendance' 'college stories'\n",
            " 'college teachers' 'comic books, strips, etc' 'comics & graphic novels'\n",
            " 'community life' 'computer programmers' 'computers' 'conduct of life'\n",
            " 'conner, rainie (fictitious character)' 'conspiracies'\n",
            " 'continental op (fictitious character)' 'cookery' 'cooking'\n",
            " 'cornwall (england : county)' 'cosmology' 'country life' 'courtship'\n",
            " 'cowboys' 'crafts & hobbies' 'crime investigations' 'criminals'\n",
            " 'crusades' 'cults' 'czech fiction' 'czech republic'\n",
            " 'dallas, eve (fictitious character)' 'dangerously mentally ill' 'death'\n",
            " 'democracy' 'demonology' 'dent, arthur (fictitious character)'\n",
            " 'detective and mystery stories' 'disasters' 'discworld (imaginary place)'\n",
            " 'divorce' 'divorced people' 'divorced women' 'domestic fiction'\n",
            " 'dominican republic' 'dracula, count (fictitious character)' 'drama'\n",
            " 'dreams' 'drenai (imaginary place)' 'dublin (ireland)'\n",
            " 'dysfunctional families' 'dystopias' 'education' 'elves'\n",
            " 'emotional problems' 'england'\n",
            " 'englisch - geschichte - lyrik - aufsatzsammlung' 'english'\n",
            " 'english drama' 'english fiction' 'english poetry' 'epic literature'\n",
            " 'epidemics' 'eretz israel' 'escapes' 'espionage'\n",
            " 'essentialism (philosophy)' 'experimental fiction' 'fairy tales'\n",
            " 'fairy tales, english' 'families' 'family' 'family & relationships'\n",
            " 'family life' 'fantasy' 'fantasy fiction' 'fantasy fiction, american'\n",
            " 'fantasy fiction, english' 'farm life' 'feature films [dvd]'\n",
            " 'female friendship' 'fiction' 'first loves' 'french drama' 'friendship'\n",
            " 'friendship in adolescence' 'gambling' 'games' 'gardening'\n",
            " 'ghost stories, american' 'girls' 'good and evil' 'great britain'\n",
            " 'handicapped youth' 'health & fitness' 'historical fiction' 'history'\n",
            " 'history, modern' 'horror stories.' 'horror tales'\n",
            " 'horror tales, american' 'horror tales, english' 'house & home'\n",
            " 'human cloning' 'human-alien encounters' 'human-animal relationships'\n",
            " 'humor' 'humorous fiction' 'humorous stories' 'identity (psychology)'\n",
            " 'igbo (african people)' 'indic fiction (english)' 'intelligence service'\n",
            " 'interplanetary voyages' 'jews' 'juvenile fiction' 'juvenile nonfiction'\n",
            " 'kyoto (japan)' 'language arts & disciplines' 'latin america' 'law'\n",
            " 'life on other planets' 'lisbon (portugal)' 'literary collections'\n",
            " 'literary criticism' 'literature' 'little, stuart (fictitious character)'\n",
            " 'london (england)' 'los angeles (calif.)' 'male friendship'\n",
            " 'man-woman relationships' 'manuscripts'\n",
            " 'marple, jane (fictitious character)' 'married people' 'mathematics'\n",
            " 'medical' 'meditation' 'men' 'mental illness' 'mentally ill'\n",
            " 'mormon fundamentalism' 'mothers and sons'\n",
            " 'motion picture actors and actresses' 'motion pictures' 'murder' 'music'\n",
            " 'mythology, classical' 'napoleonic wars, 1800-1815' 'nature'\n",
            " 'new york (state)' 'nineteen ninety-eight, a.d.' 'occult fiction'\n",
            " 'organized crime' 'outlaws' \"partition of decedents' estates\"\n",
            " 'performing arts' 'persian gulf war, 1991' 'pets' 'philosophy'\n",
            " 'photographers' 'photography' 'physics' 'pigeons' 'poetry'\n",
            " 'poets, chilean' 'political fiction' 'political leadership'\n",
            " 'political science' 'prisoners of war' 'provence (france)'\n",
            " 'psycho (motion picture : 1960)' 'psychology' 'reference' 'religion'\n",
            " 'ryan, jack (fictitious character)' 'science' 'science fiction'\n",
            " 'science fiction, american' 'science fiction, english' 'sea stories'\n",
            " 'seduction' 'self-help' 'selling' 'short stories' 'slave insurrections'\n",
            " 'social action' 'social science' 'sports & recreation' 'study aids'\n",
            " 'surealism' 'survival' 'sweet valley (imaginary place)' 'television'\n",
            " 'travel' 'true crime' 'united states' 'vampires' 'war' 'women terrorists'\n",
            " 'world war, 1914-1918' 'young adult fiction' 'zero (the number)']\n",
            "Training model...\n",
            "Loading data...\n",
            "Unique values in 'categories': ['fiction' 'english fiction' 'juvenile fiction' 'gambling'\n",
            " 'biography & autobiography' 'animals, mythical' 'england'\n",
            " 'young adult fiction' 'mental illness' 'comics & graphic novels'\n",
            " 'epidemics' \"alzheimer's disease\" 'dangerously mentally ill' 'history'\n",
            " 'horror tales' 'book burning' 'health & fitness' 'china'\n",
            " 'performing arts' 'dent, arthur (fictitious character)'\n",
            " 'american fiction' 'chocolate' 'adultery' 'british' 'allegories'\n",
            " 'education' 'experimental fiction' 'true crime' 'airplane crash survival'\n",
            " 'social science' 'blind' 'arthurian romances' 'provence (france)'\n",
            " 'juvenile nonfiction' 'humor' 'brothers' 'boston (mass.)' 'death'\n",
            " 'ryan, jack (fictitious character)' 'vampires' 'domestic fiction'\n",
            " 'male friendship' 'art' \"children's stories\" 'dystopias' 'drama'\n",
            " 'alienation (social psychology)' 'study aids' 'businessmen' 'religion'\n",
            " 'mentally ill' 'college attendance' 'adventure stories' 'murder'\n",
            " 'escapes' 'castle rock (me. : imaginary place)' 'aging' 'boys' 'bombings'\n",
            " 'cookery' 'poetry' 'london (england)' 'african americans'\n",
            " 'literary criticism' 'literary collections' 'human-animal relationships'\n",
            " 'discworld (imaginary place)' \"children's literature, french\"\n",
            " 'business & economics' 'zero (the number)' 'fantasy fiction'\n",
            " 'mothers and sons' 'cowboys' 'little, stuart (fictitious character)'\n",
            " 'horror stories.' 'crafts & hobbies' 'manuscripts'\n",
            " 'conner, rainie (fictitious character)' 'emotional problems' 'men'\n",
            " 'body, mind & spirit' 'fairy tales' 'demonology'\n",
            " 'detective and mystery stories' 'courtship' 'latin america' 'english'\n",
            " 'classical fiction' 'philosophy' 'authors, english' 'occult fiction'\n",
            " 'atonement' 'christmas' 'divorced people' 'african american men'\n",
            " 'self-help' 'science fiction' 'brewing' 'bullying' 'fantasy'\n",
            " 'fantasy fiction, english' 'black death' 'science' 'criminals'\n",
            " 'life on other planets' 'family & relationships' 'travel' 'americans'\n",
            " 'amish' 'photography' 'american literature' 'language arts & disciplines'\n",
            " 'authors, american' 'angels' 'air pilots' 'science fiction, english'\n",
            " 'computers' 'assassins' 'dracula, count (fictitious character)'\n",
            " 'capitalism' 'beowulf' 'dreams' 'christian life' 'computer programmers'\n",
            " 'political science' 'poets, chilean' 'world war, 1914-1918' 'psychology'\n",
            " 'candy' 'political fiction' 'dominican republic' 'arithmetic'\n",
            " 'family life' 'bible' 'first loves' 'children of the rich' 'accidents'\n",
            " 'aged women' 'divorced women' 'humorous fiction' 'pigeons'\n",
            " 'napoleonic wars, 1800-1815' 'conspiracies' 'elves' 'families' 'authors'\n",
            " 'dallas, eve (fictitious character)' 'handicapped youth'\n",
            " 'baggins, frodo (fictitious character)' 'alcoholics' 'disasters'\n",
            " 'lisbon (portugal)' 'city and town life' 'czech fiction'\n",
            " 'intelligence service' 'friendship' 'farm life' 'college teachers'\n",
            " 'kyoto (japan)' 'college stories' 'nature' 'new york (state)'\n",
            " 'photographers' 'pets' 'espionage' 'surealism' 'girls'\n",
            " 'essentialism (philosophy)' 'organized crime' 'women terrorists'\n",
            " 'igbo (african people)' 'adult education' 'canterbury (england)'\n",
            " 'characters and characteristics in motion pictures' 'abused wives'\n",
            " 'medical' 'english drama' 'sea stories'\n",
            " 'englisch - geschichte - lyrik - aufsatzsammlung' 'los angeles (calif.)'\n",
            " 'american wit and humour' 'belgians' 'reference'\n",
            " 'indic fiction (english)' 'family' 'austria' 'epic literature' 'cancer'\n",
            " 'banks and banking, british' 'adolescence' 'persian gulf war, 1991'\n",
            " 'eretz israel' 'alternative histories (fiction)' 'ballet' 'anger' 'music'\n",
            " 'african american plantation owners' 'cooking' 'business women'\n",
            " \"partition of decedents' estates\" 'clergy' 'black humor (literature)'\n",
            " 'feature films [dvd]' 'fairy tales, english' 'botswana'\n",
            " 'beresford, tommy (fictitious character)' 'benedictine monasteries'\n",
            " 'interplanetary voyages' 'selling' 'cosmology' 'catholic women'\n",
            " 'comic books, strips, etc' 'short stories' 'human cloning'\n",
            " 'african american families' 'literature' 'authors, cuban' 'law'\n",
            " 'female friendship' 'seduction' 'authors, italian' 'humorous stories'\n",
            " 'american poetry' 'divorce' 'christianity' 'war'\n",
            " 'bond, james (fictitious character)' 'slave insurrections' 'apprentices'\n",
            " 'political leadership' 'democracy' 'dublin (ireland)'\n",
            " 'cerebrovascular disease' 'chick lit' 'friendship in adolescence'\n",
            " 'ghost stories, american' 'bereavement' 'mythology, classical'\n",
            " 'conduct of life' 'united states' 'actresses' 'birthparents'\n",
            " 'czech republic' 'identity (psychology)'\n",
            " 'marple, jane (fictitious character)' 'dysfunctional families'\n",
            " 'human-alien encounters' 'battle, superintendent (fictitious character)'\n",
            " 'community life' 'fantasy fiction, american'\n",
            " 'sweet valley (imaginary place)' 'american essays' 'arctic regions'\n",
            " 'australia' 'psycho (motion picture : 1960)' \"children's plays\"\n",
            " \"children's stories, english\" 'babytime resource' 'mormon fundamentalism'\n",
            " 'country life' 'gardening' 'crusades' 'actors' 'prisoners of war'\n",
            " 'archaeological expeditions' 'meditation' 'motion pictures'\n",
            " 'great britain' 'authors, german' 'crime investigations' 'cults' 'canada'\n",
            " 'outlaws' 'azerbaijan' 'african americans in radio broadcasting'\n",
            " 'horror tales, english' 'businesswomen' 'autism'\n",
            " 'drenai (imaginary place)' 'motion picture actors and actresses'\n",
            " 'television' 'good and evil' 'house & home' 'science fiction, american'\n",
            " 'survival' 'buddhism' 'auschwitz (poland : concentration camp)'\n",
            " 'mathematics' 'cornwall (england : county)' 'sports & recreation'\n",
            " 'american drama' 'horror tales, american' 'christmas stories'\n",
            " 'man-woman relationships' 'jews' 'history, modern' 'badgers'\n",
            " 'boarding schools' 'french drama' 'aeneas (legendary character)'\n",
            " 'boarding school-fiction' 'married people'\n",
            " 'continental op (fictitious character)' 'english poetry'\n",
            " 'boats and boating' 'nineteen ninety-eight, a.d.' 'brothers and sisters'\n",
            " 'children of divorced parents' 'social action' 'bus travel' 'games'\n",
            " 'physics' 'bail bond agents' 'historical fiction' 'Unknown']\n",
            "Remaining classes after reindexing: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
            " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287]\n",
            "Fitting the model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\code\\python\\python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\code\\python\\python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\code\\python\\python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       229\n",
            "           1       0.00      0.00      0.00        16\n",
            "           2       0.00      0.00      0.00         6\n",
            "           3       0.00      0.00      0.00         3\n",
            "           4       0.00      0.00      0.00         1\n",
            "           5       0.00      0.00      0.00         2\n",
            "           6       0.00      0.00      0.00        64\n",
            "           7       0.00      0.00      0.00       239\n",
            "           8       0.00      0.00      0.00         1\n",
            "           9       0.00      0.00      0.00         1\n",
            "          10       0.00      0.00      0.00        17\n",
            "          11       0.00      0.00      0.00         4\n",
            "          12       0.00      0.00      0.00        33\n",
            "          13       0.00      0.00      0.00         1\n",
            "          14       0.00      0.00      0.00        29\n",
            "          15       0.00      0.00      0.00        16\n",
            "          16       0.00      0.00      0.00         3\n",
            "          17       0.00      0.00      0.00        52\n",
            "          18       0.00      0.00      0.00         6\n",
            "          19       0.00      0.00      0.00        25\n",
            "          20       0.00      0.00      0.00        37\n",
            "          21       0.00      0.00      0.00         6\n",
            "          22       0.00      0.00      0.00        33\n",
            "          23       0.00      0.00      0.00         1\n",
            "          25       0.00      0.00      0.00       137\n",
            "          26       0.00      0.00      0.00        16\n",
            "          27       0.00      0.00      0.00         1\n",
            "          28       0.00      0.00      0.00         3\n",
            "          29       0.00      0.00      0.00        75\n",
            "          30       0.00      0.00      0.00        22\n",
            "          31       0.00      0.00      0.00         3\n",
            "          32       0.00      0.00      0.00         4\n",
            "          33       0.00      0.00      0.00         4\n",
            "          34       0.00      0.00      0.00         2\n",
            "          35       0.00      0.00      0.00         1\n",
            "          36       0.00      0.00      0.00         3\n",
            "          37       0.00      0.00      0.00        14\n",
            "          38       0.00      0.00      0.00        18\n",
            "          39       0.00      0.00      0.00         1\n",
            "          40       0.00      0.00      0.00         4\n",
            "          41       0.00      0.00      0.00         1\n",
            "          42       0.00      0.00      0.00         1\n",
            "          43       0.00      0.00      0.00         3\n",
            "          44       0.00      0.00      0.00        18\n",
            "          45       0.00      0.00      0.00        28\n",
            "          46       0.00      0.00      0.00         2\n",
            "          47       0.00      0.00      0.00        15\n",
            "          48       0.00      0.00      0.00         3\n",
            "          50       0.00      0.00      0.00         1\n",
            "          51       0.00      0.00      0.00        28\n",
            "          52       0.00      0.00      0.00         3\n",
            "          53       0.00      0.00      0.00        22\n",
            "          54       0.00      0.00      0.00         2\n",
            "          55       0.00      0.00      0.00         2\n",
            "          56       0.00      0.00      0.00         2\n",
            "          57       0.00      0.00      0.00        15\n",
            "          58       0.00      0.00      0.00         1\n",
            "          59       0.00      0.00      0.00         2\n",
            "          60       0.00      0.00      0.00        12\n",
            "          61       0.00      0.00      0.00       434\n",
            "          62       0.00      0.00      0.00         1\n",
            "          63       0.00      0.00      0.00        10\n",
            "          64       0.00      0.00      0.00         6\n",
            "          65       0.00      0.00      0.00        11\n",
            "          66       0.00      0.00      0.00         1\n",
            "          67       0.00      0.00      0.00        59\n",
            "          68       0.00      0.00      0.00        11\n",
            "          70       0.00      0.00      0.00       164\n",
            "          71       0.00      0.00      0.00        66\n",
            "          72       0.00      0.00      0.00        27\n",
            "          73       0.00      0.00      0.00        15\n",
            "          74       0.00      0.00      0.00         2\n",
            "          75       0.00      0.00      0.00        40\n",
            "          78       0.00      0.00      0.00         1\n",
            "          79       0.00      0.00      0.00        34\n",
            "          80       0.00      0.00      0.00        11\n",
            "          81       0.00      0.00      0.00        53\n",
            "          83       0.00      0.00      0.00         1\n",
            "          84       0.00      0.00      0.00         3\n",
            "          85       0.00      0.00      0.00         1\n",
            "          86       0.00      0.00      0.00         3\n",
            "          87       0.00      0.00      0.00        29\n",
            "          88       0.00      0.00      0.00        27\n",
            "          89       0.00      0.00      0.00         4\n",
            "          90       0.00      0.00      0.00         1\n",
            "          91       0.00      0.00      0.00         8\n",
            "          92       0.00      0.00      0.00         2\n",
            "          94       0.00      0.00      0.00        11\n",
            "          95       0.00      0.00      0.00        28\n",
            "          97       0.00      0.00      0.00        77\n",
            "          99       0.00      0.00      0.00        80\n",
            "         100       0.00      0.00      0.00        69\n",
            "         101       0.00      0.00      0.00         5\n",
            "         102       0.00      0.00      0.00         2\n",
            "         103       0.00      0.00      0.00        70\n",
            "         104       0.00      0.00      0.00         1\n",
            "         105       0.00      0.00      0.00        12\n",
            "         106       0.00      0.00      0.00        16\n",
            "         107       0.00      0.00      0.00         3\n",
            "         108       0.00      0.00      0.00        50\n",
            "         109       0.00      0.00      0.00         2\n",
            "         110       0.00      0.00      0.00         3\n",
            "         111       0.00      0.00      0.00         3\n",
            "         112       0.00      0.00      0.00       108\n",
            "         113       0.00      0.00      0.00         1\n",
            "         114       0.00      0.00      0.00        14\n",
            "         115       0.00      0.00      0.00        49\n",
            "         116       0.00      0.00      0.00         1\n",
            "         117       0.00      0.00      0.00        33\n",
            "         118       0.00      0.00      0.00         4\n",
            "         119       0.00      0.00      0.00        42\n",
            "         120       0.00      0.00      0.00         1\n",
            "         121       0.00      0.00      0.00         3\n",
            "         123       0.00      0.00      0.00        36\n",
            "         124       0.00      0.00      0.00        10\n",
            "         125       0.00      0.00      0.00         6\n",
            "         126       0.00      0.00      0.00         6\n",
            "         128       0.00      0.00      0.00         1\n",
            "         129       0.00      0.00      0.00         5\n",
            "         130       0.00      0.00      0.00         2\n",
            "         131       0.00      0.00      0.00        19\n",
            "         132       0.00      0.00      0.00        94\n",
            "         133       0.00      0.00      0.00        39\n",
            "         134       0.00      0.00      0.00         1\n",
            "         135       0.00      0.00      0.00        13\n",
            "         136       0.00      0.00      0.00        54\n",
            "         137       0.00      0.00      0.00        23\n",
            "         138       0.00      0.00      0.00         2\n",
            "         139       0.00      0.00      0.00         8\n",
            "         140       0.00      0.00      0.00         1\n",
            "         141       0.00      0.00      0.00        35\n",
            "         142       0.00      0.00      0.00        25\n",
            "         143       0.00      0.00      0.00        26\n",
            "         144       0.00      0.00      0.00         8\n",
            "         145       0.00      0.00      0.00        16\n",
            "         146       0.00      0.00      0.00        91\n",
            "         147       0.00      0.00      0.00        19\n",
            "         148       0.00      0.00      0.00         3\n",
            "         149       0.00      0.00      0.00         2\n",
            "         150       0.00      0.00      0.00        62\n",
            "         151       0.00      0.00      0.00       107\n",
            "         152       0.00      0.00      0.00         1\n",
            "         153       0.00      0.00      0.00         5\n",
            "         154       0.00      0.00      0.00         5\n",
            "         155       0.00      0.00      0.00         5\n",
            "         156       0.00      0.00      0.00        28\n",
            "         157       0.00      0.00      0.00         1\n",
            "         158       0.00      0.00      0.00       140\n",
            "         159       0.00      0.00      0.00        11\n",
            "         160       0.00      0.00      0.00         5\n",
            "         161       0.00      0.00      0.00        10\n",
            "         162       0.00      0.00      0.00        27\n",
            "         163       0.00      0.00      0.00         5\n",
            "         164       0.00      0.00      0.00         8\n",
            "         165       0.00      0.00      0.00        23\n",
            "         166       0.00      0.00      0.00        16\n",
            "         167       0.00      0.00      0.00         1\n",
            "         168       0.00      0.00      0.00         1\n",
            "         169       0.00      0.00      0.00        10\n",
            "         170       0.00      0.00      0.00        23\n",
            "         171       0.00      0.00      0.00        14\n",
            "         172       0.00      0.00      0.00        42\n",
            "         173       0.00      0.00      0.00        49\n",
            "         174       0.00      0.00      0.00         3\n",
            "         175       0.00      0.00      0.00        37\n",
            "         176       0.00      0.00      0.00         1\n",
            "         177       0.00      0.00      0.00         6\n",
            "         178       0.00      0.00      0.00         4\n",
            "         179       0.63      1.00      0.77     12796\n",
            "         180       0.00      0.00      0.00        28\n",
            "         182       0.00      0.00      0.00         5\n",
            "         183       0.00      0.00      0.00         2\n",
            "         184       0.00      0.00      0.00         4\n",
            "         185       0.00      0.00      0.00         1\n",
            "         186       0.00      0.00      0.00         1\n",
            "         187       0.00      0.00      0.00         3\n",
            "         188       0.00      0.00      0.00         2\n",
            "         189       0.00      0.00      0.00        10\n",
            "         190       0.00      0.00      0.00        65\n",
            "         191       0.00      0.00      0.00       178\n",
            "         193       0.00      0.00      0.00        23\n",
            "         194       0.00      0.00      0.00         8\n",
            "         195       0.00      0.00      0.00         1\n",
            "         196       0.00      0.00      0.00         2\n",
            "         197       0.00      0.00      0.00         3\n",
            "         198       0.00      0.00      0.00         5\n",
            "         199       0.00      0.00      0.00        16\n",
            "         200       0.00      0.00      0.00        25\n",
            "         201       0.00      0.00      0.00         1\n",
            "         202       0.00      0.00      0.00         2\n",
            "         203       0.00      0.00      0.00         9\n",
            "         204       0.00      0.00      0.00        21\n",
            "         205       0.00      0.00      0.00        24\n",
            "         206       0.00      0.00      0.00         1\n",
            "         207       0.00      0.00      0.00      1189\n",
            "         208       0.00      0.00      0.00        82\n",
            "         209       0.00      0.00      0.00        18\n",
            "         210       0.00      0.00      0.00        19\n",
            "         211       0.00      0.00      0.00        55\n",
            "         212       0.00      0.00      0.00         1\n",
            "         213       0.00      0.00      0.00        23\n",
            "         214       0.00      0.00      0.00         2\n",
            "         215       0.00      0.00      0.00        22\n",
            "         216       0.00      0.00      0.00        93\n",
            "         217       0.00      0.00      0.00         3\n",
            "         218       0.00      0.00      0.00        15\n",
            "         219       0.00      0.00      0.00       116\n",
            "         220       0.00      0.00      0.00         4\n",
            "         221       0.00      0.00      0.00        13\n",
            "         222       0.00      0.00      0.00        15\n",
            "         223       0.00      0.00      0.00         2\n",
            "         225       0.00      0.00      0.00         1\n",
            "         226       0.00      0.00      0.00        10\n",
            "         228       0.00      0.00      0.00         1\n",
            "         229       0.00      0.00      0.00        81\n",
            "         230       0.00      0.00      0.00        23\n",
            "         231       0.00      0.00      0.00         1\n",
            "         232       0.00      0.00      0.00        28\n",
            "         233       0.00      0.00      0.00         1\n",
            "         234       0.00      0.00      0.00         2\n",
            "         235       0.00      0.00      0.00        32\n",
            "         236       0.00      0.00      0.00         4\n",
            "         237       0.00      0.00      0.00         1\n",
            "         238       0.00      0.00      0.00         6\n",
            "         239       0.00      0.00      0.00        30\n",
            "         240       0.00      0.00      0.00        12\n",
            "         242       0.00      0.00      0.00         4\n",
            "         243       0.00      0.00      0.00        12\n",
            "         245       0.00      0.00      0.00         9\n",
            "         246       0.00      0.00      0.00       153\n",
            "         247       0.00      0.00      0.00         4\n",
            "         248       0.00      0.00      0.00         6\n",
            "         249       0.00      0.00      0.00        28\n",
            "         250       0.00      0.00      0.00        11\n",
            "         251       0.00      0.00      0.00         5\n",
            "         252       0.00      0.00      0.00         1\n",
            "         253       0.00      0.00      0.00       115\n",
            "         254       0.00      0.00      0.00         3\n",
            "         255       0.00      0.00      0.00         6\n",
            "         256       0.00      0.00      0.00         3\n",
            "         257       0.00      0.00      0.00        14\n",
            "         258       0.00      0.00      0.00         1\n",
            "         259       0.00      0.00      0.00        30\n",
            "         260       0.00      0.00      0.00        21\n",
            "         261       0.00      0.00      0.00         3\n",
            "         262       0.00      0.00      0.00       102\n",
            "         263       0.00      0.00      0.00        16\n",
            "         264       0.00      0.00      0.00        35\n",
            "         265       0.00      0.00      0.00        24\n",
            "         266       0.00      0.00      0.00         7\n",
            "         267       0.00      0.00      0.00         2\n",
            "         268       0.00      0.00      0.00         1\n",
            "         269       0.00      0.00      0.00        23\n",
            "         270       0.00      0.00      0.00         3\n",
            "         271       0.00      0.00      0.00         5\n",
            "         272       0.00      0.00      0.00         1\n",
            "         273       0.00      0.00      0.00        57\n",
            "         274       0.00      0.00      0.00         1\n",
            "         275       0.00      0.00      0.00        29\n",
            "         276       0.00      0.00      0.00        10\n",
            "         277       0.00      0.00      0.00         1\n",
            "         278       0.00      0.00      0.00         2\n",
            "         279       0.00      0.00      0.00        56\n",
            "         280       0.00      0.00      0.00        77\n",
            "         281       0.00      0.00      0.00         3\n",
            "         282       0.00      0.00      0.00         1\n",
            "         284       0.00      0.00      0.00        13\n",
            "         285       0.00      0.00      0.00        16\n",
            "         286       0.00      0.00      0.00        24\n",
            "         287       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           0.63     20284\n",
            "   macro avg       0.00      0.00      0.00     20284\n",
            "weighted avg       0.40      0.63      0.49     20284\n",
            "\n",
            "Model trained successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\code\\python\\python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted genre: essentialism (philosophy)\n",
            "Top predictions: [('essentialism (philosophy)', 0.75125855), ('health & fitness', 0.063524604), ('black humor (literature)', 0.025025956)]\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Vérification du modèle après entraînement\n",
        "    print(\"Training model...\")\n",
        "    model, label_encoder = train_genre_predictor()\n",
        "    print(\"Model trained successfully.\")\n",
        "\n",
        "    # Chargez et testez avec le synopsis d'exemple\n",
        "    sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    test_synopsis = \"In a world where magic is forbidden, a young girl discovers her extraordinary powers.\"\n",
        "    predicted_genre, top_predictions = predict_genre(model, label_encoder, test_synopsis, sentence_transformer)\n",
        "\n",
        "    print(\"Predicted genre:\", predicted_genre)\n",
        "    print(\"Top predictions:\", top_predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrqzMgze4z7w"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}