{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tP8a2BgrVM-9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\code\\python\\python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRix4xXuVUt-"
      },
      "source": [
        "**Fonction : Charger et préparer les données**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YQ3Dv5aIVSq-"
      },
      "outputs": [],
      "source": [
        "def load_and_prepare_data(file_path1, file_path2):\n",
        "    df1 = pd.read_csv(file_path1, low_memory=False)\n",
        "    df2 = pd.read_csv(file_path2, low_memory=False)\n",
        "\n",
        "    df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
        "\n",
        "    # Prendre 10% des donnnées pour moins de temps de calcul\n",
        "\n",
        "    # df = df.sample(frac=0.1, random_state=42)\n",
        "\n",
        "    def clean_categories(df):\n",
        "        # Supprimer les valeurs nulles ou NaN\n",
        "        df['categories'] = df['categories'].fillna('Unknown')\n",
        "        # Forcer toutes les valeurs de 'categories' en type string\n",
        "        df['categories'] = df['categories'].astype(str).str.strip()\n",
        "        return df\n",
        "    # Nettoyer la colonne 'categories'\n",
        "    df = clean_categories(df)\n",
        "\n",
        "    print(\"Unique values in 'categories':\", df['categories'].unique())\n",
        "\n",
        "    embedding_cols = [col for col in df.columns if col.startswith('desc_embed_')]\n",
        "    X = df[embedding_cols].values\n",
        "    y = df['categories']\n",
        "\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    X = imputer.fit_transform(X)\n",
        "\n",
        "    # Encoder les étiquettes\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "    # Enregistrer l'encodeur pour une utilisation ultérieure\n",
        "    encoder_filename = \"../models/label_encoder.pkl\"\n",
        "    with open(encoder_filename, \"wb\") as f:\n",
        "        pickle.dump(label_encoder, f)\n",
        "\n",
        "    # Supprimer les classes rares\n",
        "    def remove_rare_classes(X, y, min_samples=2):\n",
        "        \"\"\"Supprime les classes ayant moins de min_samples.\"\"\"\n",
        "        class_counts = pd.Series(y).value_counts()\n",
        "        rare_classes = class_counts[class_counts < min_samples].index\n",
        "        mask = ~pd.Series(y).isin(rare_classes)\n",
        "        return X[mask], y[mask]\n",
        "\n",
        "    X, y_encoded = remove_rare_classes(X, y_encoded)\n",
        "\n",
        "    # Réindexer les étiquettes pour les rendre continues\n",
        "    unique_classes = np.unique(y_encoded)\n",
        "    remap_classes = {old: new for new, old in enumerate(unique_classes)}\n",
        "    y_encoded = np.array([remap_classes[label] for label in y_encoded])\n",
        "\n",
        "    # Vérifier les nouvelles classes\n",
        "    print(\"Remaining classes after reindexing:\", np.unique(y_encoded))\n",
        "\n",
        "    return X, y_encoded, label_encoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVo9y6VFVz_q"
      },
      "source": [
        "**Fonction : Entraîner le modèle de prédiction des genres**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4L-ey4xCVvzM"
      },
      "outputs": [],
      "source": [
        "def train_genre_predictor(X, y_encoded, model_choice):\n",
        "    # Diviser les données\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        "    )\n",
        "    # Choisir un modèle\n",
        "    if model_choice == \"Logistic Reg\":\n",
        "        model = LogisticRegression(\n",
        "            max_iter=1000, \n",
        "            multi_class='ovr'\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid model choice: {model_choice}\")\n",
        "\n",
        "    print(\"Fitting the model...\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Sauvegarder le modèle\n",
        "    model_filename = f\"../models/{model_choice}.pkl\"\n",
        "    with open(model_filename, \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "    print(f\"Model saved as {model_filename}\")\n",
        "\n",
        "    # Évaluer les performances\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"Classification report:\\n{classification_report(y_test, y_pred)}\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4cfE0aJWLH0"
      },
      "source": [
        "**Fonction : Prédire le genre d'un livre à partir de son synopsis**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sOt3PB0eWJrK"
      },
      "outputs": [],
      "source": [
        "def predict_genre(model, label_encoder, synopsis, sentence_transformer):\n",
        "    \"\"\"\n",
        "    Predict the genre of a new book based on its synopsis\n",
        "    \"\"\"\n",
        "    # Generate embedding for the new synopsis\n",
        "    synopsis_embedding = sentence_transformer.encode([synopsis])\n",
        "\n",
        "    # Make prediction\n",
        "    genre_encoded = model.predict(synopsis_embedding)\n",
        "    genre = label_encoder.inverse_transform(genre_encoded)\n",
        "\n",
        "    # Get prediction probabilities\n",
        "    proba = model.predict_proba(synopsis_embedding)\n",
        "    top_genres_idx = np.argsort(proba[0])[-3:][::-1]  # Get top 3 genres\n",
        "    top_genres = label_encoder.inverse_transform(top_genres_idx)\n",
        "    top_probas = proba[0][top_genres_idx]\n",
        "\n",
        "    return genre[0], list(zip(top_genres, top_probas))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Fonction : Charger le modèle de prédiction des genres**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_genre_predictor(model_save_path):\n",
        "    # model_save_path est le chemin direct vers le pkl du modèle\n",
        "    model_file = model_save_path\n",
        "    encoder_file = \"../models/label_encoder.pkl\"\n",
        "    \n",
        "    if not (os.path.exists(model_file) and os.path.exists(encoder_file)):\n",
        "        raise FileNotFoundError(\"Model files not found. Please train the model first.\")\n",
        "\n",
        "    with open(model_file, 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "    with open(encoder_file, 'rb') as f:\n",
        "        label_encoder = pickle.load(f)\n",
        "\n",
        "    return model, label_encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMacoGT5WVGR"
      },
      "source": [
        "**Programme principale**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Unique values in 'categories': ['fiction' 'english fiction' 'juvenile fiction' 'gambling'\n",
            " 'biography & autobiography' 'animals, mythical' 'england'\n",
            " 'young adult fiction' 'mental illness' 'comics & graphic novels'\n",
            " 'epidemics' \"alzheimer's disease\" 'dangerously mentally ill' 'history'\n",
            " 'horror tales' 'book burning' 'health & fitness' 'china'\n",
            " 'performing arts' 'dent, arthur (fictitious character)'\n",
            " 'american fiction' 'chocolate' 'adultery' 'british' 'allegories'\n",
            " 'education' 'experimental fiction' 'true crime' 'airplane crash survival'\n",
            " 'social science' 'blind' 'arthurian romances' 'provence (france)'\n",
            " 'juvenile nonfiction' 'humor' 'brothers' 'boston (mass.)' 'death'\n",
            " 'ryan, jack (fictitious character)' 'vampires' 'domestic fiction'\n",
            " 'male friendship' 'art' \"children's stories\" 'dystopias' 'drama'\n",
            " 'alienation (social psychology)' 'study aids' 'businessmen' 'religion'\n",
            " 'mentally ill' 'college attendance' 'adventure stories' 'murder'\n",
            " 'escapes' 'castle rock (me. : imaginary place)' 'aging' 'boys' 'bombings'\n",
            " 'cookery' 'poetry' 'london (england)' 'african americans'\n",
            " 'literary criticism' 'literary collections' 'human-animal relationships'\n",
            " 'discworld (imaginary place)' \"children's literature, french\"\n",
            " 'business & economics' 'zero (the number)' 'fantasy fiction'\n",
            " 'mothers and sons' 'cowboys' 'little, stuart (fictitious character)'\n",
            " 'horror stories.' 'crafts & hobbies' 'manuscripts'\n",
            " 'conner, rainie (fictitious character)' 'emotional problems' 'men'\n",
            " 'body, mind & spirit' 'fairy tales' 'demonology'\n",
            " 'detective and mystery stories' 'courtship' 'latin america' 'english'\n",
            " 'classical fiction' 'philosophy' 'authors, english' 'occult fiction'\n",
            " 'atonement' 'christmas' 'divorced people' 'african american men'\n",
            " 'self-help' 'science fiction' 'brewing' 'bullying' 'fantasy'\n",
            " 'fantasy fiction, english' 'black death' 'science' 'criminals'\n",
            " 'life on other planets' 'family & relationships' 'travel' 'americans'\n",
            " 'amish' 'photography' 'american literature' 'language arts & disciplines'\n",
            " 'authors, american' 'angels' 'air pilots' 'science fiction, english'\n",
            " 'computers' 'assassins' 'dracula, count (fictitious character)'\n",
            " 'capitalism' 'beowulf' 'dreams' 'christian life' 'computer programmers'\n",
            " 'political science' 'poets, chilean' 'world war, 1914-1918' 'psychology'\n",
            " 'candy' 'political fiction' 'dominican republic' 'arithmetic'\n",
            " 'family life' 'bible' 'first loves' 'children of the rich' 'accidents'\n",
            " 'aged women' 'divorced women' 'humorous fiction' 'pigeons'\n",
            " 'napoleonic wars, 1800-1815' 'conspiracies' 'elves' 'families' 'authors'\n",
            " 'dallas, eve (fictitious character)' 'handicapped youth'\n",
            " 'baggins, frodo (fictitious character)' 'alcoholics' 'disasters'\n",
            " 'lisbon (portugal)' 'city and town life' 'czech fiction'\n",
            " 'intelligence service' 'friendship' 'farm life' 'college teachers'\n",
            " 'kyoto (japan)' 'college stories' 'nature' 'new york (state)'\n",
            " 'photographers' 'pets' 'espionage' 'surealism' 'girls'\n",
            " 'essentialism (philosophy)' 'organized crime' 'women terrorists'\n",
            " 'igbo (african people)' 'adult education' 'canterbury (england)'\n",
            " 'characters and characteristics in motion pictures' 'abused wives'\n",
            " 'medical' 'english drama' 'sea stories'\n",
            " 'englisch - geschichte - lyrik - aufsatzsammlung' 'los angeles (calif.)'\n",
            " 'american wit and humour' 'belgians' 'reference'\n",
            " 'indic fiction (english)' 'family' 'austria' 'epic literature' 'cancer'\n",
            " 'banks and banking, british' 'adolescence' 'persian gulf war, 1991'\n",
            " 'eretz israel' 'alternative histories (fiction)' 'ballet' 'anger' 'music'\n",
            " 'african american plantation owners' 'cooking' 'business women'\n",
            " \"partition of decedents' estates\" 'clergy' 'black humor (literature)'\n",
            " 'feature films [dvd]' 'fairy tales, english' 'botswana'\n",
            " 'beresford, tommy (fictitious character)' 'benedictine monasteries'\n",
            " 'interplanetary voyages' 'selling' 'cosmology' 'catholic women'\n",
            " 'comic books, strips, etc' 'short stories' 'human cloning'\n",
            " 'african american families' 'literature' 'authors, cuban' 'law'\n",
            " 'female friendship' 'seduction' 'authors, italian' 'humorous stories'\n",
            " 'american poetry' 'divorce' 'christianity' 'war'\n",
            " 'bond, james (fictitious character)' 'slave insurrections' 'apprentices'\n",
            " 'political leadership' 'democracy' 'dublin (ireland)'\n",
            " 'cerebrovascular disease' 'chick lit' 'friendship in adolescence'\n",
            " 'ghost stories, american' 'bereavement' 'mythology, classical'\n",
            " 'conduct of life' 'united states' 'actresses' 'birthparents'\n",
            " 'czech republic' 'identity (psychology)'\n",
            " 'marple, jane (fictitious character)' 'dysfunctional families'\n",
            " 'human-alien encounters' 'battle, superintendent (fictitious character)'\n",
            " 'community life' 'fantasy fiction, american'\n",
            " 'sweet valley (imaginary place)' 'american essays' 'arctic regions'\n",
            " 'australia' 'psycho (motion picture : 1960)' \"children's plays\"\n",
            " \"children's stories, english\" 'babytime resource' 'mormon fundamentalism'\n",
            " 'country life' 'gardening' 'crusades' 'actors' 'prisoners of war'\n",
            " 'archaeological expeditions' 'meditation' 'motion pictures'\n",
            " 'great britain' 'authors, german' 'crime investigations' 'cults' 'canada'\n",
            " 'outlaws' 'azerbaijan' 'african americans in radio broadcasting'\n",
            " 'horror tales, english' 'businesswomen' 'autism'\n",
            " 'drenai (imaginary place)' 'motion picture actors and actresses'\n",
            " 'television' 'good and evil' 'house & home' 'science fiction, american'\n",
            " 'survival' 'buddhism' 'auschwitz (poland : concentration camp)'\n",
            " 'mathematics' 'cornwall (england : county)' 'sports & recreation'\n",
            " 'american drama' 'horror tales, american' 'christmas stories'\n",
            " 'man-woman relationships' 'jews' 'history, modern' 'badgers'\n",
            " 'boarding schools' 'french drama' 'aeneas (legendary character)'\n",
            " 'boarding school-fiction' 'married people'\n",
            " 'continental op (fictitious character)' 'english poetry'\n",
            " 'boats and boating' 'nineteen ninety-eight, a.d.' 'brothers and sisters'\n",
            " 'children of divorced parents' 'social action' 'bus travel' 'games'\n",
            " 'physics' 'bail bond agents' 'historical fiction' 'Unknown']\n",
            "Remaining classes after reindexing: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
            " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287]\n"
          ]
        }
      ],
      "source": [
        "# Charger les données\n",
        "file_path1 = '../data/final_data_part1.csv'\n",
        "file_path2 = '../data/final_data_part2.csv'\n",
        "\n",
        "# Charger les données\n",
        "print(\"Loading data...\")\n",
        "X, y_encoded, label_encoder = load_and_prepare_data(file_path1, file_path2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model...\n",
            "Fitting the model...\n",
            "Model saved as ../models/Logistic Reg.pkl\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       229\n",
            "           1       0.00      0.00      0.00        16\n",
            "           2       0.00      0.00      0.00         6\n",
            "           3       0.00      0.00      0.00         3\n",
            "           4       0.00      0.00      0.00         1\n",
            "           5       0.00      0.00      0.00         2\n",
            "           6       0.00      0.00      0.00        64\n",
            "           7       0.00      0.00      0.00       239\n",
            "           8       0.00      0.00      0.00         1\n",
            "           9       0.00      0.00      0.00         1\n",
            "          10       0.00      0.00      0.00        17\n",
            "          11       0.00      0.00      0.00         4\n",
            "          12       0.00      0.00      0.00        33\n",
            "          13       0.00      0.00      0.00         1\n",
            "          14       0.00      0.00      0.00        29\n",
            "          15       0.00      0.00      0.00        16\n",
            "          16       0.00      0.00      0.00         3\n",
            "          17       0.00      0.00      0.00        52\n",
            "          18       0.00      0.00      0.00         6\n",
            "          19       0.00      0.00      0.00        25\n",
            "          20       0.00      0.00      0.00        37\n",
            "          21       0.00      0.00      0.00         6\n",
            "          22       0.00      0.00      0.00        33\n",
            "          23       0.00      0.00      0.00         1\n",
            "          25       0.00      0.00      0.00       137\n",
            "          26       0.00      0.00      0.00        16\n",
            "          27       0.00      0.00      0.00         1\n",
            "          28       0.00      0.00      0.00         3\n",
            "          29       0.00      0.00      0.00        75\n",
            "          30       0.00      0.00      0.00        22\n",
            "          31       0.00      0.00      0.00         3\n",
            "          32       0.00      0.00      0.00         4\n",
            "          33       0.00      0.00      0.00         4\n",
            "          34       0.00      0.00      0.00         2\n",
            "          35       0.00      0.00      0.00         1\n",
            "          36       0.00      0.00      0.00         3\n",
            "          37       0.00      0.00      0.00        14\n",
            "          38       0.00      0.00      0.00        18\n",
            "          39       0.00      0.00      0.00         1\n",
            "          40       0.00      0.00      0.00         4\n",
            "          41       0.00      0.00      0.00         1\n",
            "          42       0.00      0.00      0.00         1\n",
            "          43       0.00      0.00      0.00         3\n",
            "          44       0.00      0.00      0.00        18\n",
            "          45       0.00      0.00      0.00        28\n",
            "          46       0.00      0.00      0.00         2\n",
            "          47       0.00      0.00      0.00        15\n",
            "          48       0.00      0.00      0.00         3\n",
            "          50       0.00      0.00      0.00         1\n",
            "          51       0.00      0.00      0.00        28\n",
            "          52       0.00      0.00      0.00         3\n",
            "          53       0.00      0.00      0.00        22\n",
            "          54       0.00      0.00      0.00         2\n",
            "          55       0.00      0.00      0.00         2\n",
            "          56       0.00      0.00      0.00         2\n",
            "          57       0.00      0.00      0.00        15\n",
            "          58       0.00      0.00      0.00         1\n",
            "          59       0.00      0.00      0.00         2\n",
            "          60       0.00      0.00      0.00        12\n",
            "          61       0.00      0.00      0.00       434\n",
            "          62       0.00      0.00      0.00         1\n",
            "          63       0.00      0.00      0.00        10\n",
            "          64       0.00      0.00      0.00         6\n",
            "          65       0.00      0.00      0.00        11\n",
            "          66       0.00      0.00      0.00         1\n",
            "          67       0.00      0.00      0.00        59\n",
            "          68       0.00      0.00      0.00        11\n",
            "          70       0.00      0.00      0.00       164\n",
            "          71       0.00      0.00      0.00        66\n",
            "          72       0.00      0.00      0.00        27\n",
            "          73       0.00      0.00      0.00        15\n",
            "          74       0.00      0.00      0.00         2\n",
            "          75       0.00      0.00      0.00        40\n",
            "          78       0.00      0.00      0.00         1\n",
            "          79       0.00      0.00      0.00        34\n",
            "          80       0.00      0.00      0.00        11\n",
            "          81       0.00      0.00      0.00        53\n",
            "          83       0.00      0.00      0.00         1\n",
            "          84       0.00      0.00      0.00         3\n",
            "          85       0.00      0.00      0.00         1\n",
            "          86       0.00      0.00      0.00         3\n",
            "          87       0.00      0.00      0.00        29\n",
            "          88       0.00      0.00      0.00        27\n",
            "          89       0.00      0.00      0.00         4\n",
            "          90       0.00      0.00      0.00         1\n",
            "          91       0.00      0.00      0.00         8\n",
            "          92       0.00      0.00      0.00         2\n",
            "          94       0.00      0.00      0.00        11\n",
            "          95       0.00      0.00      0.00        28\n",
            "          97       0.00      0.00      0.00        77\n",
            "          99       0.00      0.00      0.00        80\n",
            "         100       0.00      0.00      0.00        69\n",
            "         101       0.00      0.00      0.00         5\n",
            "         102       0.00      0.00      0.00         2\n",
            "         103       0.00      0.00      0.00        70\n",
            "         104       0.00      0.00      0.00         1\n",
            "         105       0.00      0.00      0.00        12\n",
            "         106       0.00      0.00      0.00        16\n",
            "         107       0.00      0.00      0.00         3\n",
            "         108       0.00      0.00      0.00        50\n",
            "         109       0.00      0.00      0.00         2\n",
            "         110       0.00      0.00      0.00         3\n",
            "         111       0.00      0.00      0.00         3\n",
            "         112       0.00      0.00      0.00       108\n",
            "         113       0.00      0.00      0.00         1\n",
            "         114       0.00      0.00      0.00        14\n",
            "         115       0.00      0.00      0.00        49\n",
            "         116       0.00      0.00      0.00         1\n",
            "         117       0.00      0.00      0.00        33\n",
            "         118       0.00      0.00      0.00         4\n",
            "         119       0.00      0.00      0.00        42\n",
            "         120       0.00      0.00      0.00         1\n",
            "         121       0.00      0.00      0.00         3\n",
            "         123       0.00      0.00      0.00        36\n",
            "         124       0.00      0.00      0.00        10\n",
            "         125       0.00      0.00      0.00         6\n",
            "         126       0.00      0.00      0.00         6\n",
            "         128       0.00      0.00      0.00         1\n",
            "         129       0.00      0.00      0.00         5\n",
            "         130       0.00      0.00      0.00         2\n",
            "         131       0.00      0.00      0.00        19\n",
            "         132       0.00      0.00      0.00        94\n",
            "         133       0.00      0.00      0.00        39\n",
            "         134       0.00      0.00      0.00         1\n",
            "         135       0.00      0.00      0.00        13\n",
            "         136       0.00      0.00      0.00        54\n",
            "         137       0.00      0.00      0.00        23\n",
            "         138       0.00      0.00      0.00         2\n",
            "         139       0.00      0.00      0.00         8\n",
            "         140       0.00      0.00      0.00         1\n",
            "         141       0.00      0.00      0.00        35\n",
            "         142       0.00      0.00      0.00        25\n",
            "         143       0.00      0.00      0.00        26\n",
            "         144       0.00      0.00      0.00         8\n",
            "         145       0.00      0.00      0.00        16\n",
            "         146       0.00      0.00      0.00        91\n",
            "         147       0.00      0.00      0.00        19\n",
            "         148       0.00      0.00      0.00         3\n",
            "         149       0.00      0.00      0.00         2\n",
            "         150       0.00      0.00      0.00        62\n",
            "         151       0.00      0.00      0.00       107\n",
            "         152       0.00      0.00      0.00         1\n",
            "         153       0.00      0.00      0.00         5\n",
            "         154       0.00      0.00      0.00         5\n",
            "         155       0.00      0.00      0.00         5\n",
            "         156       0.00      0.00      0.00        28\n",
            "         157       0.00      0.00      0.00         1\n",
            "         158       0.00      0.00      0.00       140\n",
            "         159       0.00      0.00      0.00        11\n",
            "         160       0.00      0.00      0.00         5\n",
            "         161       0.00      0.00      0.00        10\n",
            "         162       0.00      0.00      0.00        27\n",
            "         163       0.00      0.00      0.00         5\n",
            "         164       0.00      0.00      0.00         8\n",
            "         165       0.00      0.00      0.00        23\n",
            "         166       0.00      0.00      0.00        16\n",
            "         167       0.00      0.00      0.00         1\n",
            "         168       0.00      0.00      0.00         1\n",
            "         169       0.00      0.00      0.00        10\n",
            "         170       0.00      0.00      0.00        23\n",
            "         171       0.00      0.00      0.00        14\n",
            "         172       0.00      0.00      0.00        42\n",
            "         173       0.00      0.00      0.00        49\n",
            "         174       0.00      0.00      0.00         3\n",
            "         175       0.00      0.00      0.00        37\n",
            "         176       0.00      0.00      0.00         1\n",
            "         177       0.00      0.00      0.00         6\n",
            "         178       0.00      0.00      0.00         4\n",
            "         179       0.63      1.00      0.77     12796\n",
            "         180       0.00      0.00      0.00        28\n",
            "         182       0.00      0.00      0.00         5\n",
            "         183       0.00      0.00      0.00         2\n",
            "         184       0.00      0.00      0.00         4\n",
            "         185       0.00      0.00      0.00         1\n",
            "         186       0.00      0.00      0.00         1\n",
            "         187       0.00      0.00      0.00         3\n",
            "         188       0.00      0.00      0.00         2\n",
            "         189       0.00      0.00      0.00        10\n",
            "         190       0.00      0.00      0.00        65\n",
            "         191       0.00      0.00      0.00       178\n",
            "         193       0.00      0.00      0.00        23\n",
            "         194       0.00      0.00      0.00         8\n",
            "         195       0.00      0.00      0.00         1\n",
            "         196       0.00      0.00      0.00         2\n",
            "         197       0.00      0.00      0.00         3\n",
            "         198       0.00      0.00      0.00         5\n",
            "         199       0.00      0.00      0.00        16\n",
            "         200       0.00      0.00      0.00        25\n",
            "         201       0.00      0.00      0.00         1\n",
            "         202       0.00      0.00      0.00         2\n",
            "         203       0.00      0.00      0.00         9\n",
            "         204       0.00      0.00      0.00        21\n",
            "         205       0.00      0.00      0.00        24\n",
            "         206       0.00      0.00      0.00         1\n",
            "         207       0.00      0.00      0.00      1189\n",
            "         208       0.00      0.00      0.00        82\n",
            "         209       0.00      0.00      0.00        18\n",
            "         210       0.00      0.00      0.00        19\n",
            "         211       0.00      0.00      0.00        55\n",
            "         212       0.00      0.00      0.00         1\n",
            "         213       0.00      0.00      0.00        23\n",
            "         214       0.00      0.00      0.00         2\n",
            "         215       0.00      0.00      0.00        22\n",
            "         216       0.00      0.00      0.00        93\n",
            "         217       0.00      0.00      0.00         3\n",
            "         218       0.00      0.00      0.00        15\n",
            "         219       0.00      0.00      0.00       116\n",
            "         220       0.00      0.00      0.00         4\n",
            "         221       0.00      0.00      0.00        13\n",
            "         222       0.00      0.00      0.00        15\n",
            "         223       0.00      0.00      0.00         2\n",
            "         225       0.00      0.00      0.00         1\n",
            "         226       0.00      0.00      0.00        10\n",
            "         228       0.00      0.00      0.00         1\n",
            "         229       0.00      0.00      0.00        81\n",
            "         230       0.00      0.00      0.00        23\n",
            "         231       0.00      0.00      0.00         1\n",
            "         232       0.00      0.00      0.00        28\n",
            "         233       0.00      0.00      0.00         1\n",
            "         234       0.00      0.00      0.00         2\n",
            "         235       0.00      0.00      0.00        32\n",
            "         236       0.00      0.00      0.00         4\n",
            "         237       0.00      0.00      0.00         1\n",
            "         238       0.00      0.00      0.00         6\n",
            "         239       0.00      0.00      0.00        30\n",
            "         240       0.00      0.00      0.00        12\n",
            "         242       0.00      0.00      0.00         4\n",
            "         243       0.00      0.00      0.00        12\n",
            "         245       0.00      0.00      0.00         9\n",
            "         246       0.00      0.00      0.00       153\n",
            "         247       0.00      0.00      0.00         4\n",
            "         248       0.00      0.00      0.00         6\n",
            "         249       0.00      0.00      0.00        28\n",
            "         250       0.00      0.00      0.00        11\n",
            "         251       0.00      0.00      0.00         5\n",
            "         252       0.00      0.00      0.00         1\n",
            "         253       0.00      0.00      0.00       115\n",
            "         254       0.00      0.00      0.00         3\n",
            "         255       0.00      0.00      0.00         6\n",
            "         256       0.00      0.00      0.00         3\n",
            "         257       0.00      0.00      0.00        14\n",
            "         258       0.00      0.00      0.00         1\n",
            "         259       0.00      0.00      0.00        30\n",
            "         260       0.00      0.00      0.00        21\n",
            "         261       0.00      0.00      0.00         3\n",
            "         262       0.00      0.00      0.00       102\n",
            "         263       0.00      0.00      0.00        16\n",
            "         264       0.00      0.00      0.00        35\n",
            "         265       0.00      0.00      0.00        24\n",
            "         266       0.00      0.00      0.00         7\n",
            "         267       0.00      0.00      0.00         2\n",
            "         268       0.00      0.00      0.00         1\n",
            "         269       0.00      0.00      0.00        23\n",
            "         270       0.00      0.00      0.00         3\n",
            "         271       0.00      0.00      0.00         5\n",
            "         272       0.00      0.00      0.00         1\n",
            "         273       0.00      0.00      0.00        57\n",
            "         274       0.00      0.00      0.00         1\n",
            "         275       0.00      0.00      0.00        29\n",
            "         276       0.00      0.00      0.00        10\n",
            "         277       0.00      0.00      0.00         1\n",
            "         278       0.00      0.00      0.00         2\n",
            "         279       0.00      0.00      0.00        56\n",
            "         280       0.00      0.00      0.00        77\n",
            "         281       0.00      0.00      0.00         3\n",
            "         282       0.00      0.00      0.00         1\n",
            "         284       0.00      0.00      0.00        13\n",
            "         285       0.00      0.00      0.00        16\n",
            "         286       0.00      0.00      0.00        24\n",
            "         287       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           0.63     20284\n",
            "   macro avg       0.00      0.00      0.00     20284\n",
            "weighted avg       0.40      0.63      0.49     20284\n",
            "\n",
            "Model trained successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\code\\python\\python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\code\\python\\python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\code\\python\\python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Choix du model avec une variable\n",
        "    model_choice = \"Logistic Reg\"\n",
        "    \n",
        "    # Vérification du modèle après entraînement\n",
        "    print(\"Training model...\")\n",
        "    model= train_genre_predictor(X, y_encoded, model_choice)\n",
        "    print(\"Model trained successfully.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\code\\python\\python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Genre principal: essentialism (philosophy)\n",
            "Top 3 genres probables :\n",
            "  - Genre: essentialism (philosophy) | Probabilité: 0.58\n",
            "  - Genre: health & fitness | Probabilité: 0.05\n",
            "  - Genre: belgians | Probabilité: 0.03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\code\\python\\python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
          ]
        }
      ],
      "source": [
        "model_save_path = f\"../models/{model_choice}.pkl\"\n",
        "model, label_encoder = load_genre_predictor(model_save_path)\n",
        "\n",
        "# Chargez et testez avec le synopsis d'exemple\n",
        "sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "test_synopsis = \"In a world where magic is forbidden, a young girl discovers she has extraordinary powers. She must learn to control her abilities while hiding from those who would persecute her for her gifts.\"\n",
        "\n",
        "main_genre, top_predictions = predict_genre(model, label_encoder, test_synopsis, sentence_transformer)\n",
        "\n",
        "print(f\"Genre principal: {main_genre}\")\n",
        "print(\"Top 3 genres probables :\")\n",
        "for genre, prob in top_predictions:\n",
        "    print(f\"  - Genre: {genre} | Probabilité: {prob:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
